{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Leorasaharia/agriverse/blob/main/AgriVerse_AllInOne_Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "\n",
    "# AgriVerse \u2014 One-Notebook Trainer (Vision + NER + Q&A)\n",
    "\n",
    "This notebook trains small baselines for:\n",
    "- Tomato leaf diseases (ViT; optional MobileNetV3 via `timm`)\n",
    "- Paddy diseases (ViT)\n",
    "- Hindi NER on **Naamapadam** (and optional WikiANN)\n",
    "- Agro Q&A on **AgroQA** (mt5-small)\n",
    "\n",
    "**Toggles** are at the top\u2014set `True/False` and run **top \u2192 bottom**.\n",
    "\n",
    "_Built 2025-08-09._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# RUN TOGGLES (edit here)\n",
    "# =======================\n",
    "RUN_TOMATO_VIT = True\n",
    "RUN_PADDY_VIT  = True\n",
    "RUN_NAAMAPADAM_NER = True\n",
    "RUN_WIKIANN_NER    = True  # Added RUN_WIKIANN_NER toggle\n",
    "RUN_AGROQA_QA      = True\n",
    "RUN_TOMATO_MBV3    = False  # Optional MobileNetV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Minimal installs\n",
    "!pip -q install -U datasets transformers accelerate evaluate timm torchvision sentencepiece seqeval\n",
    "\n",
    "import os, random, numpy as np, torch, inspect\n",
    "import transformers\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available?\", torch.cuda.is_available())\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Version-safe keyword for eval strategy\n",
    "from transformers import TrainingArguments\n",
    "EVAL_KWARG = \"eval_strategy\" if \"eval_strategy\" in inspect.signature(TrainingArguments.__init__).parameters else \"evaluation_strategy\"\n",
    "print(\"Using TrainingArguments kwarg:\", EVAL_KWARG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## A) Tomato \u2014 ViT (`wellCh4n/tomato-leaf-disease-image`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if RUN_TOMATO_VIT:\n",
    "    from datasets import load_dataset, Image as HFImage\n",
    "    from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "    import numpy as np, evaluate, torch\n",
    "\n",
    "    ds = load_dataset(\"wellCh4n/tomato-leaf-disease-image\")\n",
    "    IMG_COL, LAB_COL = \"image\", \"label\"\n",
    "    if not isinstance(ds[\"train\"].features[IMG_COL], HFImage):\n",
    "        ds = ds.cast_column(IMG_COL, HFImage())\n",
    "\n",
    "    labels = ds[\"train\"].features[LAB_COL].names\n",
    "    id2label = {i:l for i,l in enumerate(labels)}\n",
    "    label2id = {l:i for i,l in enumerate(labels)}\n",
    "\n",
    "    ckpt = \"google/vit-base-patch16-224-in21k\"\n",
    "    processor = AutoImageProcessor.from_pretrained(ckpt, use_fast=True)\n",
    "\n",
    "    def transform(batch):\n",
    "        out = processor(images=batch[IMG_COL], return_tensors=\"pt\")\n",
    "        out[\"labels\"] = batch[LAB_COL]\n",
    "        return out\n",
    "\n",
    "    train_ds = ds[\"train\"].shuffle(seed=SEED).select(range(min(3000, len(ds[\"train\"])))).with_transform(transform)\n",
    "    val_name = \"validation\" if \"validation\" in ds else (\"test\" if \"test\" in ds else \"train\")\n",
    "    val_ds   = ds[val_name].with_transform(transform)\n",
    "\n",
    "    model = AutoModelForImageClassification.from_pretrained(\n",
    "        ckpt,\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"/content/tomato_vit\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=2,\n",
    "        save_strategy=\"no\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        logging_steps=50,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        **{EVAL_KWARG: \"epoch\"}\n",
    "    )\n",
    "\n",
    "    acc = evaluate.load(\"accuracy\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels_np = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        return acc.compute(predictions=preds, references=labels_np)\n",
    "\n",
    "    trainer = Trainer(model=model, args=args,\n",
    "                      train_dataset=train_ds, eval_dataset=val_ds,\n",
    "                      compute_metrics=compute_metrics)\n",
    "    trainer.train(); trainer.evaluate()\n",
    "    model.save_pretrained(\"/content/tomato_vit/model\")\n",
    "    processor.save_pretrained(\"/content/tomato_vit/processor\")\n",
    "    print(\"Saved ViT tomato model to /content/tomato_vit\")\n",
    "else:\n",
    "    print(\"Skipping Tomato ViT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### (Optional) Tomato \u2014 MobileNetV3 (timm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if RUN_TOMATO_MBV3:\n",
    "    import torch, torchvision\n",
    "    from torchvision import transforms\n",
    "    from torch.utils.data import DataLoader\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    ds = load_dataset(\"wellCh4n/tomato-leaf-disease-image\")\n",
    "    num_classes = ds[\"train\"].features[\"label\"].num_classes\n",
    "\n",
    "    tfms_train = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n",
    "    tfms_val = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor()])\n",
    "\n",
    "    def to_torch(ex, train=True):\n",
    "        x = ex[\"image\"].convert(\"RGB\")\n",
    "        x = tfms_train(x) if train else tfms_val(x)\n",
    "        return {\"pixel_values\": x, \"labels\": ex[\"label\"]}\n",
    "\n",
    "    train_torch = ds[\"train\"].shuffle(seed=SEED).select(range(min(3000, len(ds[\"train\"]))))        .with_transform(lambda b: {\"pixel_values\": torch.stack([to_torch(x, True)[\"pixel_values\"] for x in b]), \"labels\": torch.tensor(b[\"label\"])})\n",
    "    val_split = \"validation\" if \"validation\" in ds else \"test\"\n",
    "    val_torch = ds[val_split]        .with_transform(lambda b: {\"pixel_values\": torch.stack([to_torch(x, False)[\"pixel_values\"] for x in b]), \"labels\": torch.tensor(b[\"label\"])} )\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    train_loader = DataLoader(train_torch, batch_size=32, shuffle=True)\n",
    "    val_loader   = DataLoader(val_torch, batch_size=64, shuffle=False)\n",
    "\n",
    "    import timm\n",
    "    model = timm.create_model(\"mobilenetv3_small_100\", pretrained=True, num_classes=num_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def epoch(dl, train=True):\n",
    "        model.train() if train else model.eval()\n",
    "        total=0; correct=0; loss_sum=0.0\n",
    "        for batch in dl:\n",
    "            x = batch[\"pixel_values\"].to(device); y = batch[\"labels\"].to(device)\n",
    "            with torch.set_grad_enabled(train):\n",
    "                logits = model(x); loss = loss_fn(logits, y)\n",
    "                if train: opt.zero_grad(); loss.backward(); opt.step()\n",
    "            preds = logits.argmax(1)\n",
    "            total += y.size(0); correct += (preds==y).sum().item(); loss_sum += loss.item()*y.size(0)\n",
    "        return loss_sum/total, correct/total\n",
    "\n",
    "    for ep in range(2):\n",
    "        tr_loss, tr_acc = epoch(train_loader, True)\n",
    "        va_loss, va_acc = epoch(val_loader, False)\n",
    "        print(f\"[EP{ep+1}] train acc={tr_acc:.3f} val acc={va_acc:.3f}\")\n",
    "\n",
    "    os.makedirs(\"/content/tomato_mbv3\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), \"/content/tomato_mbv3/mobilenetv3_small_100.pth\")\n",
    "    print(\"Saved MobileNetV3 weights to /content/tomato_mbv3\")\n",
    "else:\n",
    "    print(\"Skipping Tomato MobileNetV3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## B) Paddy \u2014 ViT (`anthony2261/paddy-disease-classification`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if RUN_PADDY_VIT:\n",
    "    from datasets import load_dataset, Image as HFImage\n",
    "    from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "    import numpy as np, evaluate, torch\n",
    "\n",
    "    ds = load_dataset(\"anthony2261/paddy-disease-classification\")\n",
    "    IMG_COL, LAB_COL = \"image\", \"label\"\n",
    "    if not isinstance(ds[\"train\"].features[IMG_COL], HFImage):\n",
    "        ds = ds.cast_column(IMG_COL, HFImage())\n",
    "\n",
    "    labels = ds[\"train\"].features[LAB_COL].names\n",
    "    id2label = {i:l for i,l in enumerate(labels)}\n",
    "    label2id = {l:i for i,l in enumerate(labels)}\n",
    "\n",
    "    ckpt = \"google/vit-base-patch16-224-in21k\"\n",
    "    processor = AutoImageProcessor.from_pretrained(ckpt, use_fast=True)\n",
    "\n",
    "    def transform(batch):\n",
    "        out = processor(images=batch[IMG_COL], return_tensors=\"pt\")\n",
    "        out[\"labels\"] = batch[LAB_COL]\n",
    "        return out\n",
    "\n",
    "    train_ds = ds[\"train\"].shuffle(seed=SEED).select(range(min(4000, len(ds[\"train\"])))).with_transform(transform)\n",
    "    val_name = \"validation\" if \"validation\" in ds else (\"test\" if \"test\" in ds else \"train\")\n",
    "    val_ds   = ds[val_name].with_transform(transform)\n",
    "\n",
    "    model = AutoModelForImageClassification.from_pretrained(\n",
    "        ckpt, num_labels=len(labels), id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    from transformers import TrainingArguments\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"/content/paddy_vit\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=2,\n",
    "        save_strategy=\"no\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        logging_steps=50,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        **{EVAL_KWARG: \"epoch\"}\n",
    "    )\n",
    "\n",
    "    acc = evaluate.load(\"accuracy\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels_np = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        return acc.compute(predictions=preds, references=labels_np)\n",
    "\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds, compute_metrics=compute_metrics)\n",
    "    trainer.train(); trainer.evaluate()\n",
    "\n",
    "    model.save_pretrained(\"/content/paddy_vit/model\")\n",
    "    processor.save_pretrained(\"/content/paddy_vit/processor\")\n",
    "    print(\"Saved ViT paddy model to /content/paddy_vit\")\n",
    "else:\n",
    "    print(\"Skipping Paddy ViT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## C) NER \u2014 Naamapadam (Hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roll back to datasets 2.x (supports loading scripts)\n",
    "!pip -q install \"datasets==2.15\" \"evaluate<0.5\"\n",
    "\n",
    "import datasets, evaluate, os, sys\n",
    "print(\"datasets:\", datasets.__version__, \"| evaluate:\", evaluate.__version__)\n",
    "\n",
    "# hard-restart runtime so the older version is actually used\n",
    "os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    ")\n",
    "import numpy as np, evaluate, torch, inspect\n",
    "\n",
    "# Load dataset (v2.x supports dataset scripts)\n",
    "ds = load_dataset(\"ai4bharat/naamapadam\", \"hi\")\n",
    "TOK_COL, LAB_COL = \"tokens\", \"ner_tags\"\n",
    "\n",
    "label_list = ds[\"train\"].features[LAB_COL].feature.names\n",
    "id2label = {i:l for i,l in enumerate(label_list)}\n",
    "label2id = {l:i for i,l in enumerate(label_list)}\n",
    "\n",
    "base = \"xlm-roberta-base\"\n",
    "tok = AutoTokenizer.from_pretrained(base)\n",
    "\n",
    "def tokenize_and_align_labels(batch):\n",
    "    tokenized = tok(batch[TOK_COL], truncation=True, is_split_into_words=True)\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(batch[LAB_COL]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        prev = None; ids = []\n",
    "        for wid in word_ids:\n",
    "            if wid is None: ids.append(-100)\n",
    "            elif wid != prev: ids.append(labels[wid])\n",
    "            else: ids.append(labels[wid])  # label_all_tokens=True\n",
    "            prev = wid\n",
    "        new_labels.append(ids)\n",
    "    tokenized[\"labels\"] = new_labels\n",
    "    return tokenized\n",
    "\n",
    "SEED = 42\n",
    "train_raw = ds[\"train\"].shuffle(seed=SEED).select(range(min(6000, len(ds[\"train\"]))))\n",
    "val_raw   = ds[\"validation\"] if \"validation\" in ds else ds[\"test\"]\n",
    "\n",
    "cols = ds[\"train\"].column_names\n",
    "train_ds = train_raw.map(tokenize_and_align_labels, batched=True, remove_columns=cols)\n",
    "val_ds   = val_raw.map(tokenize_and_align_labels, batched=True, remove_columns=cols)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    base, num_labels=len(label_list), id2label=id2label, label2id=label2id\n",
    ")\n",
    "collator = DataCollatorForTokenClassification(tok)\n",
    "\n",
    "# transformers v4/v5 eval kwarg\n",
    "from transformers import TrainingArguments\n",
    "import inspect\n",
    "eval_kw = \"eval_strategy\" if \"eval_strategy\" in inspect.signature(TrainingArguments.__init__).parameters else \"evaluation_strategy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/naamapadam_hi_ner\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    logging_steps=50,\n",
    "    **{eval_kw: \"epoch\"}\n",
    ")\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    true_preds, true_labels = [], []\n",
    "    for p, l in zip(preds, labels):\n",
    "        p_tags, l_tags = [], []\n",
    "        for pi, li in zip(p, l):\n",
    "            if li != -100:\n",
    "                p_tags.append(label_list[pi]); l_tags.append(label_list[li])\n",
    "        true_preds.append(p_tags); true_labels.append(l_tags)\n",
    "    res = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "    return {\"precision\": res.get(\"overall_precision\", 0.0),\n",
    "            \"recall\":    res.get(\"overall_recall\", 0.0),\n",
    "            \"f1\":        res.get(\"overall_f1\", 0.0),\n",
    "            \"accuracy\":  res.get(\"overall_accuracy\", 0.0)}\n",
    "\n",
    "trainer = Trainer(model=model, args=args,\n",
    "                  train_dataset=train_ds, eval_dataset=val_ds,\n",
    "                  data_collator=collator, tokenizer=tok,\n",
    "                  compute_metrics=compute_metrics)\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "model.save_pretrained(\"/content/naamapadam_hi_ner/model\")\n",
    "tok.save_pretrained(\"/content/naamapadam_hi_ner/tokenizer\")\n",
    "print(\"Saved to /content/naamapadam_hi_ner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### (Optional) NER \u2014 WikiANN (Hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if RUN_WIKIANN_NER:\n",
    "    from datasets import load_dataset\n",
    "    from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "    import numpy as np, evaluate, torch\n",
    "\n",
    "    ds = load_dataset(\"unimelb-nlp/wikiann\", \"hi\")\n",
    "    TOK_COL, LAB_COL = \"tokens\", \"ner_tags\"\n",
    "    label_list = ds[\"train\"].features[LAB_COL].feature.names\n",
    "    id2label = {i:l for i,l in enumerate(label_list)}\n",
    "    label2id = {l:i for i,l in enumerate(label_list)}\n",
    "\n",
    "    base = \"xlm-roberta-base\"\n",
    "    tok = AutoTokenizer.from_pretrained(base)\n",
    "\n",
    "    def tokenize_and_align_labels(batch):\n",
    "        tokenized = tok(batch[TOK_COL], truncation=True, is_split_into_words=True)\n",
    "        new_labels = []\n",
    "        for i, labels in enumerate(batch[LAB_COL]):\n",
    "            word_ids = tokenized.word_ids(batch_index=i)\n",
    "            prev = None; ids = []\n",
    "            for wid in word_ids:\n",
    "                if wid is None: ids.append(-100)\n",
    "                elif wid != prev: ids.append(labels[wid])\n",
    "                else: ids.append(labels[wid])\n",
    "                prev = wid\n",
    "            new_labels.append(ids)\n",
    "        tokenized[\"labels\"] = new_labels\n",
    "        return tokenized\n",
    "\n",
    "    train_raw = ds[\"train\"].shuffle(seed=SEED).select(range(min(6000, len(ds[\"train\"]))))\n",
    "    val_raw   = ds[\"validation\"] if \"validation\" in ds else ds[\"test\"]\n",
    "\n",
    "    cols = ds[\"train\"].column_names\n",
    "    train_ds = train_raw.map(tokenize_and_align_labels, batched=True, remove_columns=cols)\n",
    "    val_ds   = val_raw.map(tokenize_and_align_labels, batched=True, remove_columns=cols)\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(base, num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
    "    collator = DataCollatorForTokenClassification(tok)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"/content/wikiann_hi_ner\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=3e-5,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"no\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        logging_steps=50,\n",
    "        **{EVAL_KWARG: \"epoch\"}\n",
    "    )\n",
    "\n",
    "    seqeval = evaluate.load(\"seqeval\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        true_preds, true_labels = [], []\n",
    "        for p, l in zip(preds, labels):\n",
    "            p_tags, l_tags = [], []\n",
    "            for pi, li in zip(p, l):\n",
    "                if li != -100:\n",
    "                    p_tags.append(label_list[pi]); l_tags.append(label_list[li])\n",
    "            true_preds.append(p_tags); true_labels.append(l_tags)\n",
    "        res = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "        return {\"precision\": res.get(\"overall_precision\", 0.0),\n",
    "                \"recall\": res.get(\"overall_recall\", 0.0),\n",
    "                \"f1\": res.get(\"overall_f1\", 0.0),\n",
    "                \"accuracy\": res.get(\"overall_accuracy\", 0.0)}\n",
    "\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds,\n",
    "                      data_collator=collator, tokenizer=tok, compute_metrics=compute_metrics)\n",
    "    trainer.train(); trainer.evaluate()\n",
    "\n",
    "    model.save_pretrained(\"/content/wikiann_hi_ner/model\")\n",
    "    tok.save_pretrained(\"/content/wikiann_hi_ner/tokenizer\")\n",
    "    print(\"Saved WikiANN NER to /content/wikiann_hi_ner\")\n",
    "else:\n",
    "    print(\"Skipping WikiANN NER\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## D) AgroQA \u2014 mt5-small (Q&A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_AGROQA_QA:\n",
    "    from datasets import load_dataset\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "    import numpy as np, torch\n",
    "\n",
    "    ds = load_dataset(\"Rahulrayudu/AgroQA\")\n",
    "    ds = ds[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
    "\n",
    "    model_ckpt = \"google/mt5-small\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "    src_field = \"Question\"; tgt_field = \"Answer\"\n",
    "    assert src_field in ds[\"train\"].column_names and tgt_field in ds[\"train\"].column_names, ds[\"train\"].column_names\n",
    "\n",
    "    def to_text(batch):\n",
    "        src = [\"question: \" + q for q in batch[src_field]]\n",
    "        # Ensure tgt is a list of strings\n",
    "        tgt = [str(a) for a in batch[tgt_field]]\n",
    "        model_in = tok(src, truncation=True)\n",
    "        with tok.as_target_tokenizer():\n",
    "            labels = tok(tgt, truncation=True)\n",
    "        model_in[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_in\n",
    "\n",
    "    tok_train = ds[\"train\"].select(range(min(2000, len(ds[\"train\"]))))        .map(to_text, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "    tok_val   = ds[\"test\"].map(to_text, batched=True, remove_columns=ds[\"test\"].column_names)\n",
    "\n",
    "    qa_model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\n",
    "    collator = DataCollatorForSeq2Seq(tok, model=qa_model)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"/content/agroqa_mt5\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=3e-4,\n",
    "        num_train_epochs=2,\n",
    "        save_strategy=\"no\",\n",
    "        # predict_with_generate=True, # Removed unsupported argument\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        logging_steps=50,\n",
    "        **{EVAL_KWARG: \"epoch\"}\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=qa_model, args=args, tokenizer=tok, data_collator=collator,\n",
    "                      train_dataset=tok_train, eval_dataset=tok_val)\n",
    "    trainer.train(); trainer.evaluate()\n",
    "\n",
    "    qa_model.save_pretrained(\"/content/agroqa_mt5/model\")\n",
    "    tok.save_pretrained(\"/content/agroqa_mt5/tokenizer\")\n",
    "    print(\"Saved AgroQA model to /content/agroqa_mt5\")\n",
    "else:\n",
    "    print(\"Skipping AgroQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting + metrics\n",
    "!pip -q install matplotlib scikit-learn\n",
    "\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Image as HFImage\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "paths = [\"/content/tomato_vit\",\"/content/paddy_vit\",\"/content/naamapadam_hi_ner\",\n",
    "         \"/content/wikiann_hi_ner\",\"/content/agroqa_mt5\"]\n",
    "for p in paths:\n",
    "    print(\"\\n\", p, \"exists?\", os.path.exists(p))\n",
    "    if os.path.exists(p):\n",
    "        print(\" files:\", [os.path.basename(x) for x in glob.glob(p+\"/*\")[:10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Image as HFImage\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch\n",
    "\n",
    "proc = AutoImageProcessor.from_pretrained(\"/content/tomato_vit/processor\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"/content/tomato_vit/model\").eval()\n",
    "\n",
    "ds = load_dataset(\"wellCh4n/tomato-leaf-disease-image\")\n",
    "im = ds[\"validation\"][0][\"image\"]                       # sample image\n",
    "inputs = proc(images=im, return_tensors=\"pt\")\n",
    "pred = model(**inputs).logits.argmax(-1).item()\n",
    "print(\"Tomato prediction:\", model.config.id2label[pred])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch, numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "# make a tiny val split\n",
    "ds = load_dataset(\"anthony2261/paddy-disease-classification\")\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "val = ds[\"test\"]\n",
    "\n",
    "proc = AutoImageProcessor.from_pretrained(\"/content/paddy_vit/processor\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"/content/paddy_vit/model\").eval()\n",
    "\n",
    "# one prediction + confidence\n",
    "im = val[0][\"image\"]\n",
    "inputs = proc(images=im, return_tensors=\"pt\")\n",
    "logits = model(**inputs).logits\n",
    "pred_id = int(logits.argmax(-1))\n",
    "probs = torch.softmax(logits, dim=-1).squeeze().tolist()\n",
    "print(\"Pred:\", model.config.id2label[pred_id], \"| conf:\", round(probs[pred_id], 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "subset = val.select(range(min(200, len(val))))\n",
    "preds = []\n",
    "for im in subset[\"image\"]:\n",
    "    p = model(**proc(images=im, return_tensors=\"pt\")).logits.argmax(-1).item()\n",
    "    preds.append(p)\n",
    "print(\"acc:\", accuracy_score(subset[\"label\"], preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "tok   = AutoTokenizer.from_pretrained(\"/content/naamapadam_hi_ner/tokenizer\", use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"/content/naamapadam_hi_ner/model\").eval()\n",
    "\n",
    "text = \"\u0930\u093e\u0939\u0941\u0932 \u0928\u0947 \u092a\u091f\u0928\u093e \u092e\u0947\u0902 \u0915\u093f\u0938\u093e\u0928 \u092e\u0947\u0932\u0947 \u0915\u093e \u0909\u0926\u094d\u0918\u093e\u091f\u0928 \u0915\u093f\u092f\u093e\u0964\"\n",
    "\n",
    "# get offsets so we can map tokens back to the original text\n",
    "enc = tok(text, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True)\n",
    "offsets = enc.pop(\"offset_mapping\")[0].tolist()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_ids = model(**enc).logits.argmax(-1)[0].tolist()\n",
    "\n",
    "labels = [model.config.id2label[i] for i in pred_ids]\n",
    "\n",
    "# merge B-/I- tags into spans\n",
    "spans = []\n",
    "current = None\n",
    "for (start, end), lab in zip(offsets, labels):\n",
    "    if start == end:  # special tokens like <s>, </s>\n",
    "        continue\n",
    "    if lab == \"O\":\n",
    "        if current: spans.append(current); current = None\n",
    "        continue\n",
    "    tag = lab.split(\"-\", 1)[-1]  # PER/LOC/ORG...\n",
    "    if current and current[\"tag\"] == tag and lab.startswith(\"I\") and start == current[\"end\"]:\n",
    "        current[\"end\"] = end\n",
    "    else:\n",
    "        if current: spans.append(current)\n",
    "        current = {\"start\": start, \"end\": end, \"tag\": tag}\n",
    "if current: spans.append(current)\n",
    "\n",
    "print([(text[s[\"start\"]:s[\"end\"]], s[\"tag\"]) for s in spans])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, pathlib\n",
    "p = \"/content/agroqa_mt5/model\"\n",
    "print(\"exists?\", os.path.exists(p))\n",
    "print(\"files:\", [os.path.basename(x) for x in glob.glob(p+\"/*\")])\n",
    "# one of these should be big:\n",
    "for fn in [\"pytorch_model.bin\",\"model.safetensors\"]:\n",
    "    f = pathlib.Path(p, fn)\n",
    "    if f.exists(): print(fn, round(f.stat().st_size/1e6,1), \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"Rahulrayudu/AgroQA\")\n",
    "print(\"GT:\", ds[\"train\"][0][\"Answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "import torch # Import torch\n",
    "\n",
    "ds = load_dataset(\"Rahulrayudu/AgroQA\"); ds = ds[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "ckpt = \"google/flan-t5-small\"\n",
    "tok = AutoTokenizer.from_pretrained(ckpt)\n",
    "def prep(b):\n",
    "    X = tok([\"question: \"+q for q in b[\"Question\"]], truncation=True)\n",
    "    with tok.as_target_tokenizer():\n",
    "        # Ensure Y is a list of strings\n",
    "        Y = tok([str(a) for a in b[\"Answer\"]], truncation=True)\n",
    "    X[\"labels\"] = Y[\"input_ids\"]; return X\n",
    "\n",
    "train = ds[\"train\"].select(range(min(5000, len(ds[\"train\"])))).map(prep, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "val   = ds[\"test\"].map(prep, batched=True, remove_columns=ds[\"test\"].column_names)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(ckpt)\n",
    "coll  = DataCollatorForSeq2Seq(tok, model=model)\n",
    "args  = TrainingArguments(\"/content/agroqa_flan\",\n",
    "    per_device_train_batch_size=8, per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2, learning_rate=3e-4, num_train_epochs=2,\n",
    "    save_strategy=\"no\", # predict_with_generate=True, # Removed unsupported argument\n",
    "    report_to=\"none\")\n",
    "Trainer(model=model, args=args, tokenizer=tok, data_collator=coll,\n",
    "        train_dataset=train, eval_dataset=val).train()\n",
    "model.save_pretrained(\"/content/agroqa_flan/model\"); tok.save_pretrained(\"/content/agroqa_flan/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = AutoTokenizer.from_pretrained(\"/content/agroqa_flan/tokenizer\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/agroqa_flan/model\").eval()\n",
    "def ask_flan(q):\n",
    "    out = model.generate(**tok(\"question: \"+q, return_tensors=\"pt\"),\n",
    "                         max_new_tokens=80, num_beams=4)\n",
    "    print(tok.decode(out[0], skip_special_tokens=True))\n",
    "ask_flan(\"When should I irrigate wheat during winter?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== ONE-CELL GRADIO DEMO WITH PATH FIX ====\n",
    "!pip -q install gradio pillow\n",
    "\n",
    "import gradio as gr, torch, os, glob\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    AutoImageProcessor, AutoModelForImageClassification,\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    AutoModelForSeq2SeqLM, logging\n",
    ")\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# ---- Model root paths (edit if you saved elsewhere) ----\n",
    "TOM = \"/content/tomato_vit\"\n",
    "PAD = \"/content/paddy_vit\"\n",
    "NER = \"/content/naamapadam_hi_ner\"\n",
    "QA_FLAN = \"/content/agroqa_flan\"      # if you trained the FLAN fallback\n",
    "QA_MT5  = \"/content/agroqa_mt5\"       # otherwise mT5\n",
    "\n",
    "# (Optional) If you unzipped a bundle like /content/ava_models.zip, ensure folders exist:\n",
    "# !unzip -q /content/ava_models.zip -d /content\n",
    "\n",
    "# ---- Helper: resolve model vs processor subdirs or flat dir ----\n",
    "def _resolve_img_dirs(base_dir: str):\n",
    "    \"\"\"\n",
    "    Return (model_dir, processor_dir) whether base_dir has split subfolders\n",
    "    or a flat structure containing the files directly.\n",
    "    \"\"\"\n",
    "    base = Path(base_dir)\n",
    "    # Prefer split dirs if present\n",
    "    model_dir = base / \"model\" if (base / \"model\").exists() else base\n",
    "    proc_dir  = base / \"processor\" if (base / \"processor\").exists() else base\n",
    "\n",
    "    # Sanity checks / friendly errors\n",
    "    if not model_dir.exists():\n",
    "        raise FileNotFoundError(f\"Model folder not found at {model_dir}. Contents of {base_dir}: {os.listdir(base_dir) if base.exists() else 'MISSING'}\")\n",
    "    # processor needs preprocessor_config.json\n",
    "    if not (proc_dir / \"preprocessor_config.json\").exists():\n",
    "        # If missing, try base directly\n",
    "        if (base / \"preprocessor_config.json\").exists():\n",
    "            proc_dir = base\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"preprocessor_config.json not found in {proc_dir}. \"\n",
    "                f\"Available files: {os.listdir(proc_dir) if proc_dir.exists() else 'MISSING'}\"\n",
    "            )\n",
    "    return str(model_dir), str(proc_dir)\n",
    "\n",
    "# =======================\n",
    "# Tomato (Image classifier)\n",
    "# =======================\n",
    "tom_model_dir, tom_proc_dir = _resolve_img_dirs(TOM)\n",
    "tom_proc = AutoImageProcessor.from_pretrained(tom_proc_dir)\n",
    "tom_mod  = AutoModelForImageClassification.from_pretrained(tom_model_dir).eval()\n",
    "\n",
    "def pred_tomato(img):\n",
    "    im = img.convert(\"RGB\") if isinstance(img, Image.Image) else Image.fromarray(img).convert(\"RGB\")\n",
    "    inputs = tom_proc(images=im, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = tom_mod(**inputs).logits\n",
    "        pred = int(logits.argmax(-1))\n",
    "        conf = torch.softmax(logits, -1)[0, pred].item()\n",
    "    return f\"{tom_mod.config.id2label[pred]}  (confidence {conf:.2f})\"\n",
    "\n",
    "# =======================\n",
    "# Paddy (Image classifier)\n",
    "# =======================\n",
    "pad_model_dir, pad_proc_dir = _resolve_img_dirs(PAD)\n",
    "pad_proc = AutoImageProcessor.from_pretrained(pad_proc_dir)\n",
    "pad_mod  = AutoModelForImageClassification.from_pretrained(pad_model_dir).eval()\n",
    "\n",
    "def pred_paddy(img):\n",
    "    im = img.convert(\"RGB\") if isinstance(img, Image.Image) else Image.fromarray(img).convert(\"RGB\")\n",
    "    inputs = pad_proc(images=im, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = pad_mod(**inputs).logits\n",
    "        pred = int(logits.argmax(-1))\n",
    "        conf = torch.softmax(logits, -1)[0, pred].item()\n",
    "    return f\"{pad_mod.config.id2label[pred]}  (confidence {conf:.2f})\"\n",
    "\n",
    "# =======================\n",
    "# NER (Hindi)\n",
    "# =======================\n",
    "# Prefer split dirs; fall back to flat if needed\n",
    "ner_tok_dir = Path(NER, \"tokenizer\") if Path(NER, \"tokenizer\").exists() else Path(NER)\n",
    "ner_model_dir = Path(NER, \"model\") if Path(NER, \"model\").exists() else Path(NER)\n",
    "\n",
    "ner_tok = AutoTokenizer.from_pretrained(str(ner_tok_dir), use_fast=True)\n",
    "ner_mod = AutoModelForTokenClassification.from_pretrained(str(ner_model_dir)).eval()\n",
    "id2label = ner_mod.config.id2label\n",
    "\n",
    "def ner_hi(text):\n",
    "    enc = ner_tok(text, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True)\n",
    "    offsets = enc.pop(\"offset_mapping\")[0].tolist()\n",
    "    with torch.no_grad():\n",
    "        pred_ids = ner_mod(**enc).logits.argmax(-1)[0].tolist()\n",
    "    labels = [id2label[i] for i in pred_ids]\n",
    "\n",
    "    # Merge to spans\n",
    "    spans, cur = [], None\n",
    "    for (s, e), lab in zip(offsets, labels):\n",
    "        if s == e:  # specials\n",
    "            continue\n",
    "        if lab == \"O\":\n",
    "            if cur: spans.append(cur); cur = None\n",
    "            continue\n",
    "        tag = lab.split(\"-\", 1)[-1]\n",
    "        if cur and cur[\"tag\"] == tag and s == cur[\"end\"]:\n",
    "            cur[\"end\"] = e\n",
    "        else:\n",
    "            if cur: spans.append(cur)\n",
    "            cur = {\"start\": s, \"end\": e, \"tag\": tag}\n",
    "    if cur: spans.append(cur)\n",
    "    ents = [(text[x[\"start\"]:x[\"end\"]], x[\"tag\"]) for x in spans]\n",
    "    return ents if ents else \"No entities\"\n",
    "\n",
    "# =======================\n",
    "# Q&A (AgroQA) \u2014 prefer FLAN if available, else mT5\n",
    "# =======================\n",
    "qa_dir = QA_FLAN if Path(QA_FLAN, \"model\").exists() else QA_MT5\n",
    "qa_tok_dir = Path(qa_dir, \"tokenizer\") if Path(qa_dir, \"tokenizer\").exists() else Path(qa_dir)\n",
    "qa_model_dir = Path(qa_dir, \"model\") if Path(qa_dir, \"model\").exists() else Path(qa_dir)\n",
    "\n",
    "qa_tok = AutoTokenizer.from_pretrained(str(qa_tok_dir))\n",
    "qa_mod = AutoModelForSeq2SeqLM.from_pretrained(str(qa_model_dir)).eval()\n",
    "\n",
    "# mT5 needs safer generation settings\n",
    "qa_mod.config.pad_token_id = qa_tok.pad_token_id\n",
    "qa_mod.config.eos_token_id = qa_tok.eos_token_id\n",
    "bad = None\n",
    "if \"mt5\" in qa_tok.name_or_path.lower() or \"agroqa_mt5\" in str(qa_dir).lower():\n",
    "    qa_mod.config.decoder_start_token_id = qa_tok.pad_token_id\n",
    "    bad = [[qa_tok.convert_tokens_to_ids(f\"<extra_id_{i}>\")] for i in range(100)]\n",
    "\n",
    "def ask(q):\n",
    "    q = \"question: \" + q\n",
    "    x = qa_tok(q, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "    with torch.no_grad():\n",
    "        y = qa_mod.generate(\n",
    "            **x,\n",
    "            max_new_tokens=80,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "            bad_words_ids=bad,\n",
    "            eos_token_id=qa_tok.eos_token_id,\n",
    "            pad_token_id=qa_tok.pad_token_id\n",
    "        )\n",
    "    return qa_tok.decode(y[0], skip_special_tokens=True)\n",
    "\n",
    "# =======================\n",
    "# Gradio UI\n",
    "# =======================\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# AgriVerse (AVA) \u2014 Demo\")\n",
    "    with gr.Tab(\"Tomato disease\"):\n",
    "        img_t = gr.Image(label=\"Upload tomato leaf\")\n",
    "        out_t = gr.Textbox(label=\"Prediction\")\n",
    "        img_t.change(pred_tomato, img_t, out_t)\n",
    "        gr.Button(\"Predict\").click(pred_tomato, img_t, out_t)\n",
    "\n",
    "    with gr.Tab(\"Paddy disease\"):\n",
    "        img_p = gr.Image(label=\"Upload paddy leaf\")\n",
    "        out_p = gr.Textbox(label=\"Prediction\")\n",
    "        img_p.change(pred_paddy, img_p, out_p)\n",
    "        gr.Button(\"Predict\").click(pred_paddy, img_p, out_p)\n",
    "\n",
    "    with gr.Tab(\"NER (Hindi)\"):\n",
    "        txt_n = gr.Textbox(label=\"Type Hindi sentence\")\n",
    "        out_n = gr.HighlightedText(label=\"Entities\")\n",
    "        gr.Button(\"Extract\").click(ner_hi, txt_n, out_n)\n",
    "\n",
    "    with gr.Tab(\"Agri Q&A\"):\n",
    "        txt_q = gr.Textbox(label=\"Ask a question\")\n",
    "        out_q = gr.Textbox(label=\"Answer\")\n",
    "        gr.Button(\"Answer\").click(ask, txt_q, out_q)\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /content/ava_models\n",
    "for p in [\"/content/tomato_vit\",\"/content/paddy_vit\",\"/content/naamapadam_hi_ner\",\"/content/agroqa_mt5\",\"/content/agroqa_flan\"]:\n",
    "    import os, shutil\n",
    "    if os.path.exists(p): shutil.copytree(p, f\"/content/ava_models/{p.split('/')[-1]}\", dirs_exist_ok=True)\n",
    "!zip -qr /content/ava_models.zip /content/ava_models\n",
    "from google.colab import files; files.download(\"/content/ava_models.zip\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}