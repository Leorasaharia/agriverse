{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Leorasaharia/agriverse/blob/main/AgriVerse_AllInOne_Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba4f77",
   "metadata": {
    "id": "67ba4f77"
   },
   "source": [
    "\n",
    "# AgriVerse \u2014 One-Notebook Trainer (Vision + NER + Q&A)\n",
    "\n",
    "This notebook trains small baselines for:\n",
    "- Tomato leaf diseases (ViT; optional MobileNetV3 via `timm`)\n",
    "- Paddy diseases (ViT)\n",
    "- Hindi NER on **Naamapadam** (and optional WikiANN)\n",
    "- Agro Q&A on **AgroQA** (mt5-small)\n",
    "\n",
    "**Toggles** are at the top\u2014set `True/False` and run **top \u2192 bottom**.\n",
    "\n",
    "_Built 2025-08-09._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f819c643",
   "metadata": {
    "id": "f819c643"
   },
   "outputs": [],
   "source": [
    "# =======================\n",
    "# RUN TOGGLES (edit here)\n",
    "# =======================\n",
    "RUN_TOMATO_VIT = True\n",
    "RUN_PADDY_VIT  = True\n",
    "RUN_NAAMAPADAM_NER = True\n",
    "RUN_WIKIANN_NER    = True  # Added RUN_WIKIANN_NER toggle\n",
    "RUN_AGROQA_QA      = True\n",
    "RUN_TOMATO_MBV3    = False  # Optional MobileNetV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9c91b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89d9c91b",
    "outputId": "d6d745c8-8647-425a-ebe5-bde41987a834"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/494.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m \u001b[32m491.5/494.8 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hTransformers: 4.55.0\n",
      "Torch: 2.8.0+cu128 | CUDA available? True\n",
      "Using TrainingArguments kwarg: eval_strategy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Minimal installs\n",
    "!pip -q install -U datasets transformers accelerate evaluate timm torchvision sentencepiece seqeval\n",
    "\n",
    "import os, random, numpy as np, torch, inspect\n",
    "import transformers\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available?\", torch.cuda.is_available())\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Version-safe keyword for eval strategy\n",
    "from transformers import TrainingArguments\n",
    "EVAL_KWARG = \"eval_strategy\" if \"eval_strategy\" in inspect.signature(TrainingArguments.__init__).parameters else \"evaluation_strategy\"\n",
    "print(\"Using TrainingArguments kwarg:\", EVAL_KWARG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939e672",
   "metadata": {
    "id": "9939e672"
   },
   "source": [
    "## A) Tomato \u2014 ViT (`wellCh4n/tomato-leaf-disease-image`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75814cb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617,
     "referenced_widgets": [
      "ed0ee8342ea7492eb072389fa6194e44",
      "9182c51ac38e4569ae8a4edbe7034951",
      "e23a75224f234a0cb14b5e75b91f2e6c",
      "4413dffb501c4a66957c1cd9a7e904b5",
      "30320f1dff8f47959410c6d367442a65",
      "e51344f6d258412595455f542ed54320",
      "014b274040e149b1a77f0e9886b66728",
      "365dd0d0fcb34128a70fedbe285faeb4",
      "e70f768accac441ba0fb4126c8e0d0ce",
      "ecaa4e255cfe4b1c9dafb92b6301e915",
      "a9f14ef7600f44cd89df1f5010b22853",
      "f568b0b862924f0fa70e1d49f8905a9f",
      "dc836102126345f5a2885c850dced458",
      "1df047698076407598c262a5a9522a02",
      "bbacf496fe6d437db0e440542ce13346",
      "76ea768af1b7441e995d53190e9f4330",
      "c752c99b42ba4f0195f3fa598fe5124c",
      "a3c080d63ac2429092291803a7ef8b0f",
      "98cf725f1def46ac84127ec3a2aadfe6",
      "a3fb84bafc4f4d519ab5160ce547e08a",
      "7de3d07bd46547feb3720b375a2e5af8",
      "a36e34cc4f6240b0a5d960e035cd688d",
      "cef99a2d232d4b2bb7f522bc2e90fca2",
      "8fa331d1a6de4920aeac2681d9f39fc3",
      "30b54238933945cb9db8f9126840930c",
      "973f9de3e94646509bf1b81a1b054e07",
      "6b3dc1a6bf274b6d91bb25cccca51e7a",
      "cf05b4c0752743e18a4fb4756dee3f86",
      "4d133b0e6e93432b8249c8ed04f528f6",
      "4146d855e27d430f89b8a1c613e43c2d",
      "e338d85e1cce4f4eb9ed2792eafc057b",
      "b11a7fc940ac48c6b00556104e6bd067",
      "b6d9c751d4bc49a6826206534daf9897",
      "bb43cdb2cfca4acba9397bfeed8c8ea7",
      "d6e48acb9ebb490abd30b12fe5ad72d3",
      "29ac54d8212a4f6eb1506c8faacccb38",
      "62db5cbd454140c089397e77b29f7960",
      "b4afff7eb15845b8a288ba4b26f76fc6",
      "cad3626d31464e2ea589c119ffabf78d",
      "3d6573b76c3d49a3b14a75060e55b9d2",
      "de3b74459616421ba9257ae46c59c0cb",
      "1d97e807c6334bd0b68431977804928a",
      "ed0768cd987b4ce398c46c0d874c53ca",
      "94ae7e384b9943babbf420198f567c5c",
      "3cccc32c16b244e6b5e05b77b27074df",
      "d5379792f2ec4c0cb07f6a2b4f6d4cb4",
      "d6c6b4d688834badb928e397ab3368b7",
      "39cab6b87d71423999bbf3967df67abd",
      "dda1e76ac2cb4cfc999d5b24be711439",
      "d3478d29b39242039fd6b5cc79919399",
      "75b48039be09419493221abd4032d608",
      "c02e52ea0c3b4db5b70fd98349fd6b57",
      "9ab252466eba466ab55fc61768302120",
      "17e1c7f239e94cffbe2f62a777cf258e",
      "5de7ea2f9baa441cb9f61b416b72e805",
      "600777d3639849c697a940072b552f55",
      "b72e7bc52c394027a0a8e92452b9abcb",
      "160e1422af1a407599d00f27aaedd8b6",
      "12f9ee27c39e4aaa868459c0a440b8b0",
      "eedf4d6bce2545af8e488b4251d84ee7",
      "1319a00daf5f417fb41374a789209c95",
      "1f628627417d4538bf3e61f2f584219a",
      "6cabf3afb94347b998600eaa7d641bbe",
      "9023038ee9ff4c348790522c52d8da0a",
      "2eec11c5f7a54e529e7509d01fb19608",
      "84fec110983a427398287985d0f51501",
      "3786e3d3e7764f64952aff9cbabd0ac4",
      "071bfcb3a74f4d708250ccd1b00118da",
      "0e22eb856965447a91f31feb26b50cc6",
      "1561560ec9ad461daeb45a3a3f9ccd54",
      "45c93b0154d946d89136e5d62cf36efa",
      "83dbb81b16094da9965ff8013a0a5143",
      "b450460da08e44e0ae876ce38965019f",
      "a2d2a97cfa664408925cf7a5c5ba4a2b",
      "22012c2a904044f895d3e3636015ef38",
      "bb7dafba25f94644ae54d8a9c63a51a4",
      "34469dda6a2f48bd9472de4e65ac2da8",
      "82d3b02f02c84d8abbff3817023cefc7",
      "633bc011874741409eea2fd30230e9d8",
      "50643fc6c0ee40d19919d480df7fe5e7",
      "5744c7bf28104d6695e390bcfde12122",
      "fed55da36e0b4c729dcec9d75ce1a8ef",
      "7074c8fd36254954bba7279f64cc367f",
      "2379e558f3f1497585b790c9bb9b0f20",
      "07007dfb89c04623b6291306c6192b69",
      "cf01d49a9a6241bab476149ccd9893c9",
      "43f587e9cb3340fabe6eb880665ee0e1",
      "6b3021473fa544a686bda80b69dbd870",
      "b77579faeb834150bb52a4a08eb2a04b",
      "50bdc00285ba4194badde8300787c9da",
      "79dd99c96748485abc0a65935e8160df",
      "503cc9772b594b2580a80a7abd23d5f2",
      "b39d79f152804a85a0932e317dffd558",
      "63a534b6b4a947158edda30e6388e7b6",
      "e2d27d3c8ed84389a2f2329a4a6c8ad9",
      "132cbb43ce5443f09f15681e56cafc0a",
      "fc84e1a745664eba8bbd0054515a87b0",
      "9abe8dc98c3b40e0952ec3090d84bbb3",
      "f14451fb27ed45dbaefe67f8eadfb52c"
     ]
    },
    "id": "75814cb4",
    "outputId": "7c32d04f-9eb2-4ebb-c391-550dfd1f8d10"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed0ee8342ea7492eb072389fa6194e44"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/224M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f568b0b862924f0fa70e1d49f8905a9f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/56.5M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cef99a2d232d4b2bb7f522bc2e90fca2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/14218 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bb43cdb2cfca4acba9397bfeed8c8ea7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/3569 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cccc32c16b244e6b5e05b77b27074df"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "600777d3639849c697a940072b552f55"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3786e3d3e7764f64952aff9cbabd0ac4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "82d3b02f02c84d8abbff3817023cefc7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b77579faeb834150bb52a4a08eb2a04b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'loss': 1.6243, 'grad_norm': 1.7924984693527222, 'learning_rate': 4.348404255319149e-05, 'epoch': 0.26595744680851063}\n",
      "{'loss': 0.9265, 'grad_norm': 1.5625306367874146, 'learning_rate': 3.683510638297872e-05, 'epoch': 0.5319148936170213}\n",
      "{'loss': 0.5916, 'grad_norm': 1.9606589078903198, 'learning_rate': 3.0186170212765956e-05, 'epoch': 0.7978723404255319}\n",
      "{'eval_loss': 0.38592058420181274, 'eval_accuracy': 0.9784253292238723, 'eval_runtime': 22.9439, 'eval_samples_per_second': 155.553, 'eval_steps_per_second': 4.881, 'epoch': 1.0}\n",
      "{'loss': 0.4168, 'grad_norm': 0.8876131772994995, 'learning_rate': 2.3537234042553192e-05, 'epoch': 1.0638297872340425}\n",
      "{'loss': 0.2899, 'grad_norm': 0.8225058913230896, 'learning_rate': 1.6888297872340426e-05, 'epoch': 1.3297872340425532}\n",
      "{'loss': 0.2524, 'grad_norm': 2.6041345596313477, 'learning_rate': 1.023936170212766e-05, 'epoch': 1.5957446808510638}\n",
      "{'loss': 0.2226, 'grad_norm': 0.6838308572769165, 'learning_rate': 3.590425531914894e-06, 'epoch': 1.8617021276595744}\n",
      "{'eval_loss': 0.23699723184108734, 'eval_accuracy': 0.9854300924628747, 'eval_runtime': 19.5936, 'eval_samples_per_second': 182.151, 'eval_steps_per_second': 5.716, 'epoch': 2.0}\n",
      "{'train_runtime': 134.0057, 'train_samples_per_second': 44.774, 'train_steps_per_second': 2.806, 'train_loss': 0.5900926184146962, 'epoch': 2.0}\n",
      "{'eval_loss': 0.23699723184108734, 'eval_accuracy': 0.9854300924628747, 'eval_runtime': 19.6964, 'eval_samples_per_second': 181.2, 'eval_steps_per_second': 5.686, 'epoch': 2.0}\n",
      "Saved ViT tomato model to /content/tomato_vit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if RUN_TOMATO_VIT:\n",
    "    from datasets import load_dataset, Image as HFImage\n",
    "    from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "    import numpy as np, evaluate, torch\n",
    "\n",
    "    ds = load_dataset(\"wellCh4n/tomato-leaf-disease-image\")\n",
    "    IMG_COL, LAB_COL = \"image\", \"label\"\n",
    "    if not isinstance(ds[\"train\"].features[IMG_COL], HFImage):\n",
    "        ds = ds.cast_column(IMG_COL, HFImage())\n",
    "\n",
    "    labels = ds[\"train\"].features[LAB_COL].names\n",
    "    id2label = {i:l for i,l in enumerate(labels)}\n",
    "    label2id = {l:i for i,l in enumerate(labels)}\n",
    "\n",
    "    ckpt = \"google/vit-base-patch16-224-in21k\"\n",
    "    processor = AutoImageProcessor.from_pretrained(ckpt, use_fast=True)\n",
    "\n",
    "    def transform(batch):\n",
    "        out = processor(images=batch[IMG_COL], return_tensors=\"pt\")\n",
    "        out[\"labels\"] = batch[LAB_COL]\n",
    "        return out\n",
    "\n",
    "    train_ds = ds[\"train\"].shuffle(seed=SEED).select(range(min(3000, len(ds[\"train\"])))).with_transform(transform)\n",
    "    val_name = \"validation\" if \"validation\" in ds else (\"test\" if \"test\" in ds else \"train\")\n",
    "    val_ds   = ds[val_name].with_transform(transform)\n",
    "\n",
    "    model = AutoModelForImageClassification.from_pretrained(\n",
    "        ckpt,\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"/content/tomato_vit\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=2,\n",
    "        save_strategy=\"no\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        logging_steps=50,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        **{EVAL_KWARG: \"epoch\"}\n",
    "    )\n",
    "\n",
    "    acc = evaluate.load(\"accuracy\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels_np = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        return acc.compute(predictions=preds, references=labels_np)\n",
    "\n",
    "    trainer = Trainer(model=model, args=args,\n",
    "                      train_dataset=train_ds, eval_dataset=val_ds,\n",
    "                      compute_metrics=compute_metrics)\n",
    "    trainer.train(); trainer.evaluate()\n",
    "    model.save_pretrained(\"/content/tomato_vit/model\")\n",
    "    processor.save_pretrained(\"/content/tomato_vit/processor\")\n",
    "    print(\"Saved ViT tomato model to /content/tomato_vit\")\n",
    "else:\n",
    "    print(\"Skipping Tomato ViT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c2a984",
   "metadata": {
    "id": "e3c2a984"
   },
   "source": [
    "### (Optional) Tomato \u2014 MobileNetV3 (timm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975d87b5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "975d87b5",
    "outputId": "f0a0d7d6-dfcd-464a-b164-3fab792a49b4"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Skipping Tomato MobileNetV3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if RUN_TOMATO_MBV3:\n",
    "    import torch, torchvision\n",
    "    from torchvision import transforms\n",
    "    from torch.utils.data import DataLoader\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    ds = load_dataset(\"wellCh4n/tomato-leaf-disease-image\")\n",
    "    num_classes = ds[\"train\"].features[\"label\"].num_classes\n",
    "\n",
    "    tfms_train = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n",
    "    tfms_val = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor()])\n",
    "\n",
    "    def to_torch(ex, train=True):\n",
    "        x = ex[\"image\"].convert(\"RGB\")\n",
    "        x = tfms_train(x) if train else tfms_val(x)\n",
    "        return {\"pixel_values\": x, \"labels\": ex[\"label\"]}\n",
    "\n",
    "    train_torch = ds[\"train\"].shuffle(seed=SEED).select(range(min(3000, len(ds[\"train\"]))))        .with_transform(lambda b: {\"pixel_values\": torch.stack([to_torch(x, True)[\"pixel_values\"] for x in b]), \"labels\": torch.tensor(b[\"label\"])})\n",
    "    val_split = \"validation\" if \"validation\" in ds else \"test\"\n",
    "    val_torch = ds[val_split]        .with_transform(lambda b: {\"pixel_values\": torch.stack([to_torch(x, False)[\"pixel_values\"] for x in b]), \"labels\": torch.tensor(b[\"label\"])} )\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "    train_loader = DataLoader(train_torch, batch_size=32, shuffle=True)\n",
    "    val_loader   = DataLoader(val_torch, batch_size=64, shuffle=False)\n",
    "\n",
    "    import timm\n",
    "    model = timm.create_model(\"mobilenetv3_small_100\", pretrained=True, num_classes=num_classes)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def epoch(dl, train=True):\n",
    "        model.train() if train else model.eval()\n",
    "        total=0; correct=0; loss_sum=0.0\n",
    "        for batch in dl:\n",
    "            x = batch[\"pixel_values\"].to(device); y = batch[\"labels\"].to(device)\n",
    "            with torch.set_grad_enabled(train):\n",
    "                logits = model(x); loss = loss_fn(logits, y)\n",
    "                if train: opt.zero_grad(); loss.backward(); opt.step()\n",
    "            preds = logits.argmax(1)\n",
    "            total += y.size(0); correct += (preds==y).sum().item(); loss_sum += loss.item()*y.size(0)\n",
    "        return loss_sum/total, correct/total\n",
    "\n",
    "    for ep in range(2):\n",
    "        tr_loss, tr_acc = epoch(train_loader, True)\n",
    "        va_loss, va_acc = epoch(val_loader, False)\n",
    "        print(f\"[EP{ep+1}] train acc={tr_acc:.3f} val acc={va_acc:.3f}\")\n",
    "\n",
    "    os.makedirs(\"/content/tomato_mbv3\", exist_ok=True)\n",
    "    torch.save(model.state_dict(), \"/content/tomato_mbv3/mobilenetv3_small_100.pth\")\n",
    "    print(\"Saved MobileNetV3 weights to /content/tomato_mbv3\")\n",
    "else:\n",
    "    print(\"Skipping Tomato MobileNetV3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7caa8",
   "metadata": {
    "id": "2fd7caa8"
   },
   "source": [
    "## B) Paddy \u2014 ViT (`anthony2261/paddy-disease-classification`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69552cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "f69552cc",
    "outputId": "39deb0b2-dec7-4d25-ab93-fbdddbc8ef1e"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'RUN_PADDY_VIT' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-600989877.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mRUN_PADDY_VIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mHFImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoImageProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForImageClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RUN_PADDY_VIT' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "if RUN_PADDY_VIT:\n",
    "    from datasets import load_dataset, Image as HFImage\n",
    "    from transformers import AutoImageProcessor, AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "    import numpy as np, evaluate, torch\n",
    "\n",
    "    ds = load_dataset(\"anthony2261/paddy-disease-classification\")\n",
    "    IMG_COL, LAB_COL = \"image\", \"label\"\n",
    "    if not isinstance(ds[\"train\"].features[IMG_COL], HFImage):\n",
    "        ds = ds.cast_column(IMG_COL, HFImage())\n",
    "\n",
    "    labels = ds[\"train\"].features[LAB_COL].names\n",
    "    id2label = {i:l for i,l in enumerate(labels)}\n",
    "    label2id = {l:i for i,l in enumerate(labels)}\n",
    "\n",
    "    ckpt = \"google/vit-base-patch16-224-in21k\"\n",
    "    processor = AutoImageProcessor.from_pretrained(ckpt, use_fast=True)\n",
    "\n",
    "    def transform(batch):\n",
    "        out = processor(images=batch[IMG_COL], return_tensors=\"pt\")\n",
    "        out[\"labels\"] = batch[LAB_COL]\n",
    "        return out\n",
    "\n",
    "    train_ds = ds[\"train\"].shuffle(seed=SEED).select(range(min(4000, len(ds[\"train\"])))).with_transform(transform)\n",
    "    val_name = \"validation\" if \"validation\" in ds else (\"test\" if \"test\" in ds else \"train\")\n",
    "    val_ds   = ds[val_name].with_transform(transform)\n",
    "\n",
    "    model = AutoModelForImageClassification.from_pretrained(\n",
    "        ckpt, num_labels=len(labels), id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
    "    )\n",
    "\n",
    "    from transformers import TrainingArguments\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"/content/paddy_vit\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=5e-5,\n",
    "        num_train_epochs=2,\n",
    "        save_strategy=\"no\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        logging_steps=50,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=torch.cuda.is_available(),\n",
    "        **{EVAL_KWARG: \"epoch\"}\n",
    "    )\n",
    "\n",
    "    acc = evaluate.load(\"accuracy\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels_np = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        return acc.compute(predictions=preds, references=labels_np)\n",
    "\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds, compute_metrics=compute_metrics)\n",
    "    trainer.train(); trainer.evaluate()\n",
    "\n",
    "    model.save_pretrained(\"/content/paddy_vit/model\")\n",
    "    processor.save_pretrained(\"/content/paddy_vit/processor\")\n",
    "    print(\"Saved ViT paddy model to /content/paddy_vit\")\n",
    "else:\n",
    "    print(\"Skipping Paddy ViT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387bdd7",
   "metadata": {
    "id": "9387bdd7"
   },
   "source": [
    "## C) NER \u2014 Naamapadam (Hindi)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Roll back to datasets 2.x (supports loading scripts)\n",
    "!pip -q install \"datasets==2.15\" \"evaluate<0.5\"\n",
    "\n",
    "import datasets, evaluate, os, sys\n",
    "print(\"datasets:\", datasets.__version__, \"| evaluate:\", evaluate.__version__)\n",
    "\n",
    "# hard-restart runtime so the older version is actually used\n",
    "os.kill(os.getpid(), 9)"
   ],
   "metadata": {
    "id": "31Kq3Qy_H7nn"
   },
   "id": "31Kq3Qy_H7nn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a5cefe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "a3a5cefe",
    "outputId": "e68776d5-ab43-40cc-afeb-3d24537332af"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-996844577.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model, args=args,\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 03:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.353600</td>\n",
       "      <td>0.290639</td>\n",
       "      <td>0.720449</td>\n",
       "      <td>0.797826</td>\n",
       "      <td>0.757166</td>\n",
       "      <td>0.912574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.259800</td>\n",
       "      <td>0.276674</td>\n",
       "      <td>0.752170</td>\n",
       "      <td>0.795093</td>\n",
       "      <td>0.773036</td>\n",
       "      <td>0.917734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='421' max='421' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [421/421 00:18]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved to /content/naamapadam_hi_ner\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    ")\n",
    "import numpy as np, evaluate, torch, inspect\n",
    "\n",
    "# Load dataset (v2.x supports dataset scripts)\n",
    "ds = load_dataset(\"ai4bharat/naamapadam\", \"hi\")\n",
    "TOK_COL, LAB_COL = \"tokens\", \"ner_tags\"\n",
    "\n",
    "label_list = ds[\"train\"].features[LAB_COL].feature.names\n",
    "id2label = {i:l for i,l in enumerate(label_list)}\n",
    "label2id = {l:i for i,l in enumerate(label_list)}\n",
    "\n",
    "base = \"xlm-roberta-base\"\n",
    "tok = AutoTokenizer.from_pretrained(base)\n",
    "\n",
    "def tokenize_and_align_labels(batch):\n",
    "    tokenized = tok(batch[TOK_COL], truncation=True, is_split_into_words=True)\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(batch[LAB_COL]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        prev = None; ids = []\n",
    "        for wid in word_ids:\n",
    "            if wid is None: ids.append(-100)\n",
    "            elif wid != prev: ids.append(labels[wid])\n",
    "            else: ids.append(labels[wid])  # label_all_tokens=True\n",
    "            prev = wid\n",
    "        new_labels.append(ids)\n",
    "    tokenized[\"labels\"] = new_labels\n",
    "    return tokenized\n",
    "\n",
    "SEED = 42\n",
    "train_raw = ds[\"train\"].shuffle(seed=SEED).select(range(min(6000, len(ds[\"train\"]))))\n",
    "val_raw   = ds[\"validation\"] if \"validation\" in ds else ds[\"test\"]\n",
    "\n",
    "cols = ds[\"train\"].column_names\n",
    "train_ds = train_raw.map(tokenize_and_align_labels, batched=True, remove_columns=cols)\n",
    "val_ds   = val_raw.map(tokenize_and_align_labels, batched=True, remove_columns=cols)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    base, num_labels=len(label_list), id2label=id2label, label2id=label2id\n",
    ")\n",
    "collator = DataCollatorForTokenClassification(tok)\n",
    "\n",
    "# transformers v4/v5 eval kwarg\n",
    "from transformers import TrainingArguments\n",
    "import inspect\n",
    "eval_kw = \"eval_strategy\" if \"eval_strategy\" in inspect.signature(TrainingArguments.__init__).parameters else \"evaluation_strategy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/naamapadam_hi_ner\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    "    logging_steps=50,\n",
    "    **{eval_kw: \"epoch\"}\n",
    ")\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    true_preds, true_labels = [], []\n",
    "    for p, l in zip(preds, labels):\n",
    "        p_tags, l_tags = [], []\n",
    "        for pi, li in zip(p, l):\n",
    "            if li != -100:\n",
    "                p_tags.append(label_list[pi]); l_tags.append(label_list[li])\n",
    "        true_preds.append(p_tags); true_labels.append(l_tags)\n",
    "    res = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "    return {\"precision\": res.get(\"overall_precision\", 0.0),\n",
    "            \"recall\":    res.get(\"overall_recall\", 0.0),\n",
    "            \"f1\":        res.get(\"overall_f1\", 0.0),\n",
    "            \"accuracy\":  res.get(\"overall_accuracy\", 0.0)}\n",
    "\n",
    "trainer = Trainer(model=model, args=args,\n",
    "                  train_dataset=train_ds, eval_dataset=val_ds,\n",
    "                  data_collator=collator, tokenizer=tok,\n",
    "                  compute_metrics=compute_metrics)\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "model.save_pretrained(\"/content/naamapadam_hi_ner/model\")\n",
    "tok.save_pretrained(\"/content/naamapadam_hi_ner/tokenizer\")\n",
    "print(\"Saved to /content/naamapadam_hi_ner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c548148c",
   "metadata": {
    "id": "c548148c"
   },
   "source": [
    "### (Optional) NER \u2014 WikiANN (Hindi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0769d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277,
     "referenced_widgets": [
      "1179762f217848059b7db6973dff1fa9",
      "4bacd47322304c459c7585a58a456d45",
      "20a4de24c7b44a4d904110f4d20e80c2",
      "8e01b71a99984d8180b2b4cc4b10bc85",
      "9df95c5fee424a17949b8a49199d37ce",
      "2cde58d7e5bf44fbba9a288bd0222902",
      "aa0135ec92094cdcb34c1d7d32b01bb2",
      "8529e034e92e465280b76b04bdaff866",
      "8c31f86c5abf4d72ae1f3c6bd0db2fef",
      "5851f8c82e0c4392a807ce49008ae978",
      "e35630fb34844885b481b6b3762de544"
     ]
    },
    "id": "6a0769d9",
    "outputId": "e9a0f6f4-0588-45b6-fbfa-3e62169ac76b"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1179762f217848059b7db6973dff1fa9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-2748683299.py:71: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds,\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='626' max='626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [626/626 01:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.378700</td>\n",
       "      <td>0.339326</td>\n",
       "      <td>0.811703</td>\n",
       "      <td>0.824695</td>\n",
       "      <td>0.818147</td>\n",
       "      <td>0.895762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>0.285798</td>\n",
       "      <td>0.848702</td>\n",
       "      <td>0.859375</td>\n",
       "      <td>0.854005</td>\n",
       "      <td>0.915903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 00:00]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved WikiANN NER to /content/wikiann_hi_ner\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if RUN_WIKIANN_NER:\n",
    "    from datasets import load_dataset\n",
    "    from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
    "    import numpy as np, evaluate, torch\n",
    "\n",
    "    ds = load_dataset(\"unimelb-nlp/wikiann\", \"hi\")\n",
    "    TOK_COL, LAB_COL = \"tokens\", \"ner_tags\"\n",
    "    label_list = ds[\"train\"].features[LAB_COL].feature.names\n",
    "    id2label = {i:l for i,l in enumerate(label_list)}\n",
    "    label2id = {l:i for i,l in enumerate(label_list)}\n",
    "\n",
    "    base = \"xlm-roberta-base\"\n",
    "    tok = AutoTokenizer.from_pretrained(base)\n",
    "\n",
    "    def tokenize_and_align_labels(batch):\n",
    "        tokenized = tok(batch[TOK_COL], truncation=True, is_split_into_words=True)\n",
    "        new_labels = []\n",
    "        for i, labels in enumerate(batch[LAB_COL]):\n",
    "            word_ids = tokenized.word_ids(batch_index=i)\n",
    "            prev = None; ids = []\n",
    "            for wid in word_ids:\n",
    "                if wid is None: ids.append(-100)\n",
    "                elif wid != prev: ids.append(labels[wid])\n",
    "                else: ids.append(labels[wid])\n",
    "                prev = wid\n",
    "            new_labels.append(ids)\n",
    "        tokenized[\"labels\"] = new_labels\n",
    "        return tokenized\n",
    "\n",
    "    train_raw = ds[\"train\"].shuffle(seed=SEED).select(range(min(6000, len(ds[\"train\"]))))\n",
    "    val_raw   = ds[\"validation\"] if \"validation\" in ds else ds[\"test\"]\n",
    "\n",
    "    cols = ds[\"train\"].column_names\n",
    "    train_ds = train_raw.map(tokenize_and_align_labels, batched=True, remove_columns=cols)\n",
    "    val_ds   = val_raw.map(tokenize_and_align_labels, batched=True, remove_columns=cols)\n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(base, num_labels=len(label_list), id2label=id2label, label2id=label2id)\n",
    "    collator = DataCollatorForTokenClassification(tok)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"/content/wikiann_hi_ner\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        learning_rate=3e-5,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"no\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        logging_steps=50,\n",
    "        **{EVAL_KWARG: \"epoch\"}\n",
    "    )\n",
    "\n",
    "    seqeval = evaluate.load(\"seqeval\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        true_preds, true_labels = [], []\n",
    "        for p, l in zip(preds, labels):\n",
    "            p_tags, l_tags = [], []\n",
    "            for pi, li in zip(p, l):\n",
    "                if li != -100:\n",
    "                    p_tags.append(label_list[pi]); l_tags.append(label_list[li])\n",
    "            true_preds.append(p_tags); true_labels.append(l_tags)\n",
    "        res = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "        return {\"precision\": res.get(\"overall_precision\", 0.0),\n",
    "                \"recall\": res.get(\"overall_recall\", 0.0),\n",
    "                \"f1\": res.get(\"overall_f1\", 0.0),\n",
    "                \"accuracy\": res.get(\"overall_accuracy\", 0.0)}\n",
    "\n",
    "    trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=val_ds,\n",
    "                      data_collator=collator, tokenizer=tok, compute_metrics=compute_metrics)\n",
    "    trainer.train(); trainer.evaluate()\n",
    "\n",
    "    model.save_pretrained(\"/content/wikiann_hi_ner/model\")\n",
    "    tok.save_pretrained(\"/content/wikiann_hi_ner/tokenizer\")\n",
    "    print(\"Saved WikiANN NER to /content/wikiann_hi_ner\")\n",
    "else:\n",
    "    print(\"Skipping WikiANN NER\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb4e43",
   "metadata": {
    "id": "9cfb4e43"
   },
   "source": [
    "## D) AgroQA \u2014 mt5-small (Q&A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff5d5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 622,
     "referenced_widgets": [
      "a83c1e0b069a468ca6314d0b5f6b7a67",
      "38a031e1bc0e46829f38cfea1e5e86ee",
      "59b1959d9d244a3f89d55d4872ad232f",
      "2ba784dcc33146ffa48155a5a3d3a3e1",
      "75de22d9d31144cf80ef9ac013f60b0f",
      "7c6ecf03687d463dba0ca42b8a142221",
      "45c56f2a8ec74bd9b536c4d03a992492",
      "5ed9594f998e45519432927b84a70131",
      "f892b93e28e941e68e90c5b67631e2e5",
      "fd49f6f187b447aeacd56639199d4bc4",
      "7aa366f5bcba41bab59cd5665c962538",
      "18e88e1c51aa4665811b4fea7713905e",
      "7fac7356283d4b24a09c0ebed350b9dd",
      "6f4841bdb3f0425999f4b8d3ace49ad3",
      "5d003d2cc958482b87ddb83cb2fa32ee",
      "1b00a317d5184a8783008fdc3bb7ac64",
      "92ccf42c6a6b4dac812dc681d618af4e",
      "1f010a3866e14ded8a4bef1761fbabe7",
      "406d7aeb0c8e4281948937bd97db7a92",
      "1d0aede3d3b742779c0f845d4448a49f",
      "3ae0862560b34596bdc42bc5b881a595",
      "265fbe818ca74efbaefff2be77060d53",
      "76086d05843e48989e116e7f707116d9",
      "ce11142849404df3b7a2194b54e10ad9",
      "ec279edde19e4479bc8b19396e49f0fe",
      "a0a5c126c1704b1caecf7b0de6c3aba9",
      "c07b06548c754ac28ed33ba40cc974ea",
      "c9e1edf884544e078fb5b82e56a6a488",
      "736d236f8f44496ebd57c9c427360911",
      "31d8ff34155a4b31817be3fd7aa7e997",
      "61b185efda294b7c90e18a12557d8ab5",
      "ee1306d246564f93bc0d906f39130be8",
      "ea43390af7b84c888c72c868ba35c0d3",
      "2790686ea69f4eabb122f0667de9f274",
      "16e649b0ca7f4b919357045187e99b51",
      "fbc1795c58fa411b9d5bcf6cca900c40",
      "c37e87a16b304c37bd57ecdf7363855a",
      "77221ad8de0846a9b516b91420869be5",
      "4863b24ba4724db8952fb8d0d7039001",
      "4f9a13372dd549c7b2e568c7db7971fc",
      "835af90fd11848fb9862ee136756fd07",
      "8b865d2413d84ddf8bb5b771377c935d",
      "b5d751152e6142b994447377963d9581",
      "6ea6413d90ec4906afddda3684768ad4",
      "db6915454b1f49bba6c93cc92f932ebd",
      "146246e7b27e441fb270bef983b37d52",
      "42f613fb2cc540759b7726402f4fa91a",
      "dfd511a0a79e4c2eb4171a200a79ccbe",
      "7c1edbd9290349d8b381a8da114c2038",
      "7f9d94c4f9174ed8b44a685208203589",
      "f85932ee5485422eb53c074d2a486fe2",
      "16d2c52366254fb6ad260e6b1f9385d4",
      "cc263c8974f848ea80702dc8f929af23",
      "0d18f48dc4ca468094f9fb8bd949c3d9",
      "2e18bfcd2a0346ce9ce4da9748cfa8f6",
      "50f85011f271440cba22eebeb2d2723e",
      "9365dea05cdf44fd9ba5c7770d3d0877",
      "6f20ce1d78154861b0ee58e3acd570e3",
      "ad2b1ff8cac54ce18a47b00c95c5f95d",
      "c3b7370a77824f1d91ff353e0fa2d9a2",
      "4384ac3ce96e469688ab74d27ca2bb28",
      "81cd6e57f131455e8c54818e45f7cf8e",
      "e6ae94521f63477496c6f853e724bd97",
      "a138395ef1b04896b52c8f8b8a849ccb",
      "33c9ebb8ee714b98ab8a96ea7a771f75",
      "e959fe4060f44984aea33b7423fc2fba",
      "bfcc0eb9906545f28254d3d06a25ed83",
      "e8d6a9e1232d48039f61efeb476350cc",
      "cf724f114d1e4447a5e3a09081693668",
      "ac614d45d9c74dffaeffc2e661e09dc8",
      "4cb5c9f13e9f4879bdc9595cf829e08b",
      "69927c92cd4447f19c034a1127e87dd1",
      "2ecb8536744e4aacb8c106bd79a48708",
      "d12a5ce774bb41ed815dbe97848f8c55",
      "600b380ee200409c9cad1c78c3ab769c",
      "8cd200cc6c664027ab0cc9b32e7071de",
      "92991b79bc104dfb9100c56f3a14f7e1",
      "fe10ede2cb7b492886f2d3c0ab48c11f",
      "93c6c4e37e60481ebfdee4c0c3a5e51d",
      "03d2ecd17995416f81a5e54de2776b48",
      "6465d7c19e894687b5103b6417b55330",
      "2489e79485d447cd95bc3fc80ba19ed5",
      "7d7e5d8d5a7a4401a839e535bc70540e",
      "a35785895aff4429a4a8792fc5de9474",
      "310ea825ea33460a80e4a888b8d17885",
      "78bcac10dc9747aba11892eb64d4b7f3",
      "81f5df866f6548118393ec932baa6665",
      "2e18c1d1ac8a4b39a669311ecf390966",
      "4deebb36cea846199d68652ea9376a07",
      "3843598d7f0047bf897c31b8132f8271",
      "06bd3a694c9247ada1ecef52246e1ecf",
      "84a4cd3606484a969b027cbfedafb0d8",
      "cd592e04bb8a48cda41840f1c05def59",
      "2f24ab4ef77f4326baf6369641686cdd",
      "2cec7553fd524188905f25679d809726",
      "0fafbcbb0ed843f8bb2eb56855a89545",
      "8f072f765a8e49b1b9e6137cea26bb90",
      "98c78a2e8749447faeabe4a5ad334a30",
      "7ae46e85b2da49558c19fb9ead0f1e9e"
     ]
    },
    "id": "67ff5d5b",
    "outputId": "1fc1c43c-b625-4b00-992d-05dedc20008d"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a83c1e0b069a468ca6314d0b5f6b7a67"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18e88e1c51aa4665811b4fea7713905e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "76086d05843e48989e116e7f707116d9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2790686ea69f4eabb122f0667de9f274"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db6915454b1f49bba6c93cc92f932ebd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:4006: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "50f85011f271440cba22eebeb2d2723e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfcc0eb9906545f28254d3d06a25ed83"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.20G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe10ede2cb7b492886f2d3c0ab48c11f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4deebb36cea846199d68652ea9376a07"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-2329431911.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=qa_model, args=args, tokenizer=tok, data_collator=collator,\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:16, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39/39 00:01]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved AgroQA model to /content/agroqa_mt5\n"
     ]
    }
   ],
   "source": [
    "if RUN_AGROQA_QA:\n",
    "    from datasets import load_dataset\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "    import numpy as np, torch\n",
    "\n",
    "    ds = load_dataset(\"Rahulrayudu/AgroQA\")\n",
    "    ds = ds[\"train\"].train_test_split(test_size=0.1, seed=SEED)\n",
    "\n",
    "    model_ckpt = \"google/mt5-small\"\n",
    "    tok = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "    src_field = \"Question\"; tgt_field = \"Answer\"\n",
    "    assert src_field in ds[\"train\"].column_names and tgt_field in ds[\"train\"].column_names, ds[\"train\"].column_names\n",
    "\n",
    "    def to_text(batch):\n",
    "        src = [\"question: \" + q for q in batch[src_field]]\n",
    "        # Ensure tgt is a list of strings\n",
    "        tgt = [str(a) for a in batch[tgt_field]]\n",
    "        model_in = tok(src, truncation=True)\n",
    "        with tok.as_target_tokenizer():\n",
    "            labels = tok(tgt, truncation=True)\n",
    "        model_in[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_in\n",
    "\n",
    "    tok_train = ds[\"train\"].select(range(min(2000, len(ds[\"train\"]))))        .map(to_text, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "    tok_val   = ds[\"test\"].map(to_text, batched=True, remove_columns=ds[\"test\"].column_names)\n",
    "\n",
    "    qa_model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)\n",
    "    collator = DataCollatorForSeq2Seq(tok, model=qa_model)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=\"/content/agroqa_mt5\",\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=3e-4,\n",
    "        num_train_epochs=2,\n",
    "        save_strategy=\"no\",\n",
    "        # predict_with_generate=True, # Removed unsupported argument\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        report_to=\"none\",\n",
    "        logging_steps=50,\n",
    "        **{EVAL_KWARG: \"epoch\"}\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=qa_model, args=args, tokenizer=tok, data_collator=collator,\n",
    "                      train_dataset=tok_train, eval_dataset=tok_val)\n",
    "    trainer.train(); trainer.evaluate()\n",
    "\n",
    "    qa_model.save_pretrained(\"/content/agroqa_mt5/model\")\n",
    "    tok.save_pretrained(\"/content/agroqa_mt5/tokenizer\")\n",
    "    print(\"Saved AgroQA model to /content/agroqa_mt5\")\n",
    "else:\n",
    "    print(\"Skipping AgroQA\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# plotting + metrics\n",
    "!pip -q install matplotlib scikit-learn\n",
    "\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Image as HFImage\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import evaluate\n"
   ],
   "metadata": {
    "id": "H35MrH_Y_U8A"
   },
   "id": "H35MrH_Y_U8A",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os, glob\n",
    "paths = [\"/content/tomato_vit\",\"/content/paddy_vit\",\"/content/naamapadam_hi_ner\",\n",
    "         \"/content/wikiann_hi_ner\",\"/content/agroqa_mt5\"]\n",
    "for p in paths:\n",
    "    print(\"\\n\", p, \"exists?\", os.path.exists(p))\n",
    "    if os.path.exists(p):\n",
    "        print(\" files:\", [os.path.basename(x) for x in glob.glob(p+\"/*\")[:10]])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gV7SOSXAizH",
    "outputId": "e0ed64b4-7bd5-4029-f527-23ca52beb715"
   },
   "id": "9gV7SOSXAizH",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " /content/tomato_vit exists? True\n",
      " files: ['processor', 'model']\n",
      "\n",
      " /content/paddy_vit exists? True\n",
      " files: ['processor', 'model']\n",
      "\n",
      " /content/naamapadam_hi_ner exists? True\n",
      " files: ['tokenizer', 'model']\n",
      "\n",
      " /content/wikiann_hi_ner exists? True\n",
      " files: ['tokenizer', 'model']\n",
      "\n",
      " /content/agroqa_mt5 exists? True\n",
      " files: ['tokenizer', 'model']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, Image as HFImage\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch\n",
    "\n",
    "proc = AutoImageProcessor.from_pretrained(\"/content/tomato_vit/processor\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"/content/tomato_vit/model\").eval()\n",
    "\n",
    "ds = load_dataset(\"wellCh4n/tomato-leaf-disease-image\")\n",
    "im = ds[\"validation\"][0][\"image\"]                       # sample image\n",
    "inputs = proc(images=im, return_tensors=\"pt\")\n",
    "pred = model(**inputs).logits.argmax(-1).item()\n",
    "print(\"Tomato prediction:\", model.config.id2label[pred])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258,
     "referenced_widgets": [
      "bf6586518c1a40ccb1a3bfe87eddea02",
      "cedd0ffd0d2545759d9faf3ddd8e166a",
      "a7bba86558c74970b98e17111b943c76",
      "0c4a53ee2b554c2daac1b817c7989a2c",
      "756c39e51667409a85c4c90ec7169702",
      "a88c12ed1df8476f9ebb2559fcf9122c",
      "62a3a933eab648aca70f93d212cffe76",
      "8bd2cc14c5294f99af400a98203aa5b1",
      "bdc8dc995633472c870727975f9f828f",
      "dcafe9adf06a430abfc0eb71f3cae5b5",
      "cf0374ee659e4c35802bef8eb08141b5",
      "0ee2ddc022094cefbd25f26eac059b28",
      "e0f26e7c45b74782a8f717917b83ae72",
      "979eea410ed44a8ea7705d2f7a55581d",
      "2c704a97a1b64083a6cf28a2b1b0f180",
      "73f8fe21fea54aeeadf7ad1949be2b27",
      "99cdf7ffa77041b79f192b500498b772",
      "fde5e07374da4c0791f7245bda2abe2f",
      "1e6712d69fc042a0bac25851abc6d34c",
      "885de77b6d0c4894a52a0cdb920a95f4",
      "56802539bcaf47e0a8f9db8225e7691a",
      "cb9d60f42ab34383be55882363d897e2",
      "ec00c04129824ed0b2fbf23b26b764a2",
      "b24230c02c764ca0bee431a74e653330",
      "f33b11a33a7a4f7fbe1a745441ee7188",
      "ff516564483e42f7bebfaa1f4a458cfe",
      "a78bdaaea7804598b4c8359e35dbf354",
      "81b5b97172a54b04bd1ddd0da5a94add",
      "4561975efdd448b79adc442e615a1b58",
      "fec0e7d52f0a4a0789d64b258cb238d8",
      "3113c45535764cfb932940d4ef36635a",
      "8ca4b35d56064bd18d93b20e2dbdf258",
      "4193933e1cba497fa24a445792c2ddfa",
      "e47c592c88204e3f8b2b168981f53e7e",
      "20b282256813483c9369868c2ee30383",
      "8c575fe0b5c543e9a258e0901dd05631",
      "5bba4385286b4b16851728fbfa6b64db",
      "3c0361cfed914030ace56ca3eb74f525",
      "e5f5dd3454f742cfb52a2496becf80f7",
      "19d7c26fa7634e0dbdd54b0b9ebc1714",
      "399890d8f58d4ecba3d1a33d2dc06a73",
      "7386513ad6ea4e51822f7e050e11bec5",
      "7fd7e1420316475b9f1ccc375f05da47",
      "6049562d788a4d489633983bfb6ad448",
      "c87a688f43d44ac1b9f1c1deb094bdab",
      "38df4beced63457388e8561fb5563c3e",
      "a562a8dece2c4ebb8fbed59438b20e02",
      "fe34442d8d7e44619f786fe35bdec579",
      "32a35b871cab43c990d831c93a8aad7c",
      "f2680036a00a4212bfc90cb662ad97da",
      "0511038d476d452f94d9eb5527646b8e",
      "432f363c31f14d71b67b72f3bd937a9f",
      "d5fcd65999bd4912836394e80cbe79d1",
      "68e3ce380977474ab86e33d6910c3252",
      "06f253dbee044541ac78bfb5b96e0035",
      "d63f0dc124424d1d822f43dd819a4dc6",
      "8b81a438fdcb4d7fb95b9cee7134f8b0",
      "8d9dbb0609f04e048a2f6d83491540ae",
      "0cd37ba09d8f41298867cbb6b0a8ef6e",
      "275053a170aa4b63904136d530985b7d",
      "90e3e98a69014e75b5ca22c4428038a1",
      "0ee3144f5ddc4051b404cd1c47307cd6",
      "cb316ee31e1b4a76b9b7508947fbc9cf",
      "d666f6fb65cb438db18d9dcd83c1bc03",
      "598b1ed35fd14f9e8dbcd2f531d652dc",
      "89dc1919780e43dfb3a566f6e07eb69a",
      "3086690d021a462e8122623429d919bb",
      "785b685daf1f4b569b13e48562784a41",
      "7c18c69050254d6abe8f97b03f9b88a4",
      "8f33ce996c2f47b79eb57a2aff620a0e",
      "54dc0b5cc89a4a9fa2fc0181336e1049",
      "30a98c18fc4e443fb6f666898056da58",
      "c42248cb0452484fae50db2a66273b0e",
      "a0eab8569c714e958acaead4432f4838",
      "31beb0bd455645b38ac1c38620b0d3d8",
      "ca02e04748fe47ef81923242f334fb64",
      "fc3160111784447fbfde1485067d2820"
     ]
    },
    "id": "G6Yw1n4tDRqe",
    "outputId": "e4bc9084-3707-47f9-bccc-acc14c6006d5"
   },
   "id": "G6Yw1n4tDRqe",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf6586518c1a40ccb1a3bfe87eddea02"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ee2ddc022094cefbd25f26eac059b28"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/224M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec00c04129824ed0b2fbf23b26b764a2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading data:   0%|          | 0.00/56.5M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e47c592c88204e3f8b2b168981f53e7e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c87a688f43d44ac1b9f1c1deb094bdab"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/14218 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d63f0dc124424d1d822f43dd819a4dc6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Generating validation split:   0%|          | 0/3569 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3086690d021a462e8122623429d919bb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tomato prediction: A tomato leaf with Late Blight\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "import torch, numpy as np\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "# make a tiny val split\n",
    "ds = load_dataset(\"anthony2261/paddy-disease-classification\")\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "val = ds[\"test\"]\n",
    "\n",
    "proc = AutoImageProcessor.from_pretrained(\"/content/paddy_vit/processor\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"/content/paddy_vit/model\").eval()\n",
    "\n",
    "# one prediction + confidence\n",
    "im = val[0][\"image\"]\n",
    "inputs = proc(images=im, return_tensors=\"pt\")\n",
    "logits = model(**inputs).logits\n",
    "pred_id = int(logits.argmax(-1))\n",
    "probs = torch.softmax(logits, dim=-1).squeeze().tolist()\n",
    "print(\"Pred:\", model.config.id2label[pred_id], \"| conf:\", round(probs[pred_id], 3))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMLipU6kDWV-",
    "outputId": "698858b7-c939-4998-f26f-cd29d0a6708b"
   },
   "id": "jMLipU6kDWV-",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pred: hispa | conf: 0.734\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "subset = val.select(range(min(200, len(val))))\n",
    "preds = []\n",
    "for im in subset[\"image\"]:\n",
    "    p = model(**proc(images=im, return_tensors=\"pt\")).logits.argmax(-1).item()\n",
    "    preds.append(p)\n",
    "print(\"acc:\", accuracy_score(subset[\"label\"], preds))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bonsaw2UFVpu",
    "outputId": "66103f9f-0413-4171-b69e-c8b9c324ca7e"
   },
   "id": "bonsaw2UFVpu",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "acc: 0.97\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "tok   = AutoTokenizer.from_pretrained(\"/content/naamapadam_hi_ner/tokenizer\", use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"/content/naamapadam_hi_ner/model\").eval()\n",
    "\n",
    "text = \"\u0930\u093e\u0939\u0941\u0932 \u0928\u0947 \u092a\u091f\u0928\u093e \u092e\u0947\u0902 \u0915\u093f\u0938\u093e\u0928 \u092e\u0947\u0932\u0947 \u0915\u093e \u0909\u0926\u094d\u0918\u093e\u091f\u0928 \u0915\u093f\u092f\u093e\u0964\"\n",
    "\n",
    "# get offsets so we can map tokens back to the original text\n",
    "enc = tok(text, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True)\n",
    "offsets = enc.pop(\"offset_mapping\")[0].tolist()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_ids = model(**enc).logits.argmax(-1)[0].tolist()\n",
    "\n",
    "labels = [model.config.id2label[i] for i in pred_ids]\n",
    "\n",
    "# merge B-/I- tags into spans\n",
    "spans = []\n",
    "current = None\n",
    "for (start, end), lab in zip(offsets, labels):\n",
    "    if start == end:  # special tokens like <s>, </s>\n",
    "        continue\n",
    "    if lab == \"O\":\n",
    "        if current: spans.append(current); current = None\n",
    "        continue\n",
    "    tag = lab.split(\"-\", 1)[-1]  # PER/LOC/ORG...\n",
    "    if current and current[\"tag\"] == tag and lab.startswith(\"I\") and start == current[\"end\"]:\n",
    "        current[\"end\"] = end\n",
    "    else:\n",
    "        if current: spans.append(current)\n",
    "        current = {\"start\": start, \"end\": end, \"tag\": tag}\n",
    "if current: spans.append(current)\n",
    "\n",
    "print([(text[s[\"start\"]:s[\"end\"]], s[\"tag\"]) for s in spans])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NG1YHgTSDawV",
    "outputId": "ea1ea8c8-1058-4365-e2e6-5e9f9ede5bfd"
   },
   "id": "NG1YHgTSDawV",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('\u0930\u093e\u0939\u0941\u0932', 'PER'), ('\u092a\u091f\u0928\u093e', 'LOC')]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os, glob, pathlib\n",
    "p = \"/content/agroqa_mt5/model\"\n",
    "print(\"exists?\", os.path.exists(p))\n",
    "print(\"files:\", [os.path.basename(x) for x in glob.glob(p+\"/*\")])\n",
    "# one of these should be big:\n",
    "for fn in [\"pytorch_model.bin\",\"model.safetensors\"]:\n",
    "    f = pathlib.Path(p, fn)\n",
    "    if f.exists(): print(fn, round(f.stat().st_size/1e6,1), \"MB\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59BRhAAvDcWs",
    "outputId": "1bde2085-ec5a-49b6-d4b8-3d9d8e212404"
   },
   "id": "59BRhAAvDcWs",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "exists? True\n",
      "files: ['config.json', 'generation_config.json', 'model.safetensors']\n",
      "model.safetensors 1200.7 MB\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"Rahulrayudu/AgroQA\")\n",
    "print(\"GT:\", ds[\"train\"][0][\"Answer\"])\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8M2-a1lFofH",
    "outputId": "9c671776-8692-4c1f-f027-a50d3008ccb7"
   },
   "id": "V8M2-a1lFofH",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "GT: Machinery weeders are available\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    "import torch # Import torch\n",
    "\n",
    "ds = load_dataset(\"Rahulrayudu/AgroQA\"); ds = ds[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "ckpt = \"google/flan-t5-small\"\n",
    "tok = AutoTokenizer.from_pretrained(ckpt)\n",
    "def prep(b):\n",
    "    X = tok([\"question: \"+q for q in b[\"Question\"]], truncation=True)\n",
    "    with tok.as_target_tokenizer():\n",
    "        # Ensure Y is a list of strings\n",
    "        Y = tok([str(a) for a in b[\"Answer\"]], truncation=True)\n",
    "    X[\"labels\"] = Y[\"input_ids\"]; return X\n",
    "\n",
    "train = ds[\"train\"].select(range(min(5000, len(ds[\"train\"])))).map(prep, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "val   = ds[\"test\"].map(prep, batched=True, remove_columns=ds[\"test\"].column_names)\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(ckpt)\n",
    "coll  = DataCollatorForSeq2Seq(tok, model=model)\n",
    "args  = TrainingArguments(\"/content/agroqa_flan\",\n",
    "    per_device_train_batch_size=8, per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2, learning_rate=3e-4, num_train_epochs=2,\n",
    "    save_strategy=\"no\", # predict_with_generate=True, # Removed unsupported argument\n",
    "    report_to=\"none\")\n",
    "Trainer(model=model, args=args, tokenizer=tok, data_collator=coll,\n",
    "        train_dataset=train, eval_dataset=val).train()\n",
    "model.save_pretrained(\"/content/agroqa_flan/model\"); tok.save_pretrained(\"/content/agroqa_flan/tokenizer\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 540,
     "referenced_widgets": [
      "327d97be01e34c9290aa0e381e846102",
      "c356de0cf88b4402906c969745d70b2e",
      "b8581cbc2b094c239c99fcb0b1ad050c",
      "772c201f19a64d07a8f2ee1212dfaf23",
      "2126a149ca5f40a2a40377e6357a5831",
      "a46a4a48a7074371aa2913fa1589a09e",
      "88ea2704b50b41a9b7dd5cd057e7710c",
      "538b4abd9512466e9b1d3f18c27e99db",
      "d08c58728d934a3bb26be849af8759da",
      "39a77f0077894e108c2e40b434c6953d",
      "4b4be4c1823c43faa6ed255846fd483a",
      "9ff539773a9448ea8205defaf400807b",
      "9ca4dfc3a4a44c8ebb5de43d5a661397",
      "bef223d3d1034201ae7829c32ea0c5d3",
      "3a59817ac5314b079dedb32fb67d5ced",
      "f3b777fe78524ec8a04d22dbd1966498",
      "b473f1c2f13c41a5b76b0cf419bc9c3c",
      "1cb428028e844b409f47cdd25c43e063",
      "d0904b9058e24f02bdc2f87d573e0016",
      "70f2a2edc1654409998f694ca3273101",
      "b05a42ba86bc47fb8212ca68a18fc510",
      "91ad3dda2c7441a1b580a1c57b8d01a2",
      "b2d121573d73447e963ef42864630418",
      "b071c79f4b0546458113abcf0b9cb6a8",
      "c7733c63898c46af88672f1e69d73c15",
      "67b91d626b934f39a22ef4d34b5d0a5a",
      "13f72dc9c3004af2b4248bb413595b19",
      "47d16c3cb81942f19043821a64b20757",
      "4e027a9361ea4840865235e8b5adac50",
      "5d1b734eca7d4feeb353f106e79dfeb0",
      "280f16ce3fc1441e9bf57cec120e6ec3",
      "63638bbfaeee4adb8ddf840164ca8561",
      "22582a1468f34629b793ee924d89358f",
      "7f38a45b3e874b27ba5443107d81166e",
      "bcb7524e063f4fb5b446f2419e87e183",
      "b351cfbc3de547379a878e88771ff7c6",
      "b3fecaec5d284193afcf4a3b4e9636f1",
      "1bad1be234fa4da682ff28672c849bcd",
      "ffbe142a430e4a2883a8ba40c4a38886",
      "5d94ac0679ec4fa6b72051e474bc56b3",
      "7d6957d5b8b74fa3a1f0d425eb30337a",
      "9ffd2a6115bc499e9aa7fde8c1d42d68",
      "5df1ad25467d4f03b8565499beecc981",
      "2b880893523a4b77b6d55dc1b23ef22c",
      "08f54f790ba44f4f86f4ea341ac5d307",
      "8a1305658d0449d8960bd1031b9ef382",
      "e14c1511a39d44aebcd914c390268c6a",
      "404c84b63d4d48418ed6aa3c255b8dcf",
      "94c780e861214b929b8d869d246f0684",
      "136efb09aa274369ad85cdf5363a66f2",
      "fd64201e1a6146d3a8f2b9b2b7b2822b",
      "70f6ce6865b9463ea239a84124ba887a",
      "e6d379e1288042da8d15248229379827",
      "2106890efd9f4c969ab100d3f4d2d865",
      "b26d1af19e6d4555ba062ed44d3beb38",
      "920333759b4a4521b229bc807667a38a",
      "44546cfeb64e4db491d855c3e5b6d4f7",
      "ff047560171741748d5aaa4171265659",
      "8c962b1b462645a4aa7fc6585328d30d",
      "83bcd7712750419fa18761bcb4eacdf7",
      "c08de28fd7744216958bfa0cee44b815",
      "82d364ff6ecc4f2eb439784675cd6ec7",
      "221ef75ad4cf4bec99900f6b2bd4a463",
      "674c313fe1e64a6c915ee8f6f61af6c9",
      "e4f2b1af2e034d73bce2b6000c8d4b7a",
      "ea76c90dda9444d6b951977eba845576",
      "c95c2316bff542b2995915407ca8487f",
      "1308fbf7b4194231b654f67a10d3d201",
      "37305338cf314354a35fc368bf80aff0",
      "d0362e120cbd4aed86241798bff10a09",
      "9fabdf1ccf6b4a81afab40c8bba850cc",
      "b5b03831cee4408290ae74842efd3684",
      "e1feb084551b46fc99dbd6c1e1c4fc83",
      "1104ee19cf7e417191f082f545b4afaf",
      "871f1af2ca5e44c1a6e5a039e1400ebe",
      "5594319fad584840a9331a289319b1a9",
      "ec3ac1bd06f2498a91aefb8b6b33892b",
      "1f5e15bb22fc408fbc618c78b0defc9b",
      "e1c94c157a8c4a11be391e29422c13ae",
      "019fe282292b4be3bc1e526caffd6f63",
      "d1c92ae17042444abca1fdaba85d0d89",
      "fb49ed887f924720b772b7691f7f7865",
      "33a3bfef0ab84077afd338ce44b4cbfd",
      "d7bcf9915724490ea1201f3730862be2",
      "bd676b74ab294646a208f9c70f0054de",
      "8dc383e9659947c5a3d1ff27a00569e7",
      "1d3e915eada143718aa8d3b2b0aa3435",
      "074e480dd65d43d29b59bf3792bee8c7",
      "8e4586cc0a014b4d93588aa6adaf0ad8",
      "a36863eb3abb4329886a1f7e6cc675de",
      "ce36e6c3cd33410e91e5c56d61c8f7b0",
      "68ca7c2e343a437eaddf2b76787586e3",
      "a58b5d54eff241ccbd76b1f7922998fb",
      "68cac031994347d7aba91af3a6ab8925",
      "45f34e9f26744c02aceededa4cc19446",
      "4cf8cd8a8faa4b61aed66d4be120b7c6",
      "830b5fd387094370bf90c2e28673bf63",
      "8e2126828a044513af11019d8eeb31b7",
      "126df5a896b74bccaa6f4e98546ea82a"
     ]
    },
    "id": "aI4YeemfJhkX",
    "outputId": "80821ba3-5e1a-4580-bbe0-1791e6a04872"
   },
   "id": "aI4YeemfJhkX",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "327d97be01e34c9290aa0e381e846102"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ff539773a9448ea8205defaf400807b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2d121573d73447e963ef42864630418"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f38a45b3e874b27ba5443107d81166e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2739 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08f54f790ba44f4f86f4ea341ac5d307"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:4006: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "920333759b4a4521b229bc807667a38a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c95c2316bff542b2995915407ca8487f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f5e15bb22fc408fbc618c78b0defc9b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e4586cc0a014b4d93588aa6adaf0ad8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-3802145326.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  Trainer(model=model, args=args, tokenizer=tok, data_collator=coll,\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='344' max='344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [344/344 01:19, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('/content/agroqa_flan/tokenizer/tokenizer_config.json',\n",
       " '/content/agroqa_flan/tokenizer/special_tokens_map.json',\n",
       " '/content/agroqa_flan/tokenizer/spiece.model',\n",
       " '/content/agroqa_flan/tokenizer/added_tokens.json',\n",
       " '/content/agroqa_flan/tokenizer/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tok = AutoTokenizer.from_pretrained(\"/content/agroqa_flan/tokenizer\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/agroqa_flan/model\").eval()\n",
    "def ask_flan(q):\n",
    "    out = model.generate(**tok(\"question: \"+q, return_tensors=\"pt\"),\n",
    "                         max_new_tokens=80, num_beams=4)\n",
    "    print(tok.decode(out[0], skip_special_tokens=True))\n",
    "ask_flan(\"When should I irrigate wheat during winter?\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7iXJlHoKgOw",
    "outputId": "86c9aec9-1eae-4fe8-c42a-e95e358f133a"
   },
   "id": "u7iXJlHoKgOw",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "During the winter, it is best to irrigate wheat during the winter.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ==== ONE-CELL GRADIO DEMO WITH PATH FIX ====\n",
    "!pip -q install gradio pillow\n",
    "\n",
    "import gradio as gr, torch, os, glob\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    AutoImageProcessor, AutoModelForImageClassification,\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    AutoModelForSeq2SeqLM, logging\n",
    ")\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# ---- Model root paths (edit if you saved elsewhere) ----\n",
    "TOM = \"/content/tomato_vit\"\n",
    "PAD = \"/content/paddy_vit\"\n",
    "NER = \"/content/naamapadam_hi_ner\"\n",
    "QA_FLAN = \"/content/agroqa_flan\"      # if you trained the FLAN fallback\n",
    "QA_MT5  = \"/content/agroqa_mt5\"       # otherwise mT5\n",
    "\n",
    "# (Optional) If you unzipped a bundle like /content/ava_models.zip, ensure folders exist:\n",
    "# !unzip -q /content/ava_models.zip -d /content\n",
    "\n",
    "# ---- Helper: resolve model vs processor subdirs or flat dir ----\n",
    "def _resolve_img_dirs(base_dir: str):\n",
    "    \"\"\"\n",
    "    Return (model_dir, processor_dir) whether base_dir has split subfolders\n",
    "    or a flat structure containing the files directly.\n",
    "    \"\"\"\n",
    "    base = Path(base_dir)\n",
    "    # Prefer split dirs if present\n",
    "    model_dir = base / \"model\" if (base / \"model\").exists() else base\n",
    "    proc_dir  = base / \"processor\" if (base / \"processor\").exists() else base\n",
    "\n",
    "    # Sanity checks / friendly errors\n",
    "    if not model_dir.exists():\n",
    "        raise FileNotFoundError(f\"Model folder not found at {model_dir}. Contents of {base_dir}: {os.listdir(base_dir) if base.exists() else 'MISSING'}\")\n",
    "    # processor needs preprocessor_config.json\n",
    "    if not (proc_dir / \"preprocessor_config.json\").exists():\n",
    "        # If missing, try base directly\n",
    "        if (base / \"preprocessor_config.json\").exists():\n",
    "            proc_dir = base\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f\"preprocessor_config.json not found in {proc_dir}. \"\n",
    "                f\"Available files: {os.listdir(proc_dir) if proc_dir.exists() else 'MISSING'}\"\n",
    "            )\n",
    "    return str(model_dir), str(proc_dir)\n",
    "\n",
    "# =======================\n",
    "# Tomato (Image classifier)\n",
    "# =======================\n",
    "tom_model_dir, tom_proc_dir = _resolve_img_dirs(TOM)\n",
    "tom_proc = AutoImageProcessor.from_pretrained(tom_proc_dir)\n",
    "tom_mod  = AutoModelForImageClassification.from_pretrained(tom_model_dir).eval()\n",
    "\n",
    "def pred_tomato(img):\n",
    "    im = img.convert(\"RGB\") if isinstance(img, Image.Image) else Image.fromarray(img).convert(\"RGB\")\n",
    "    inputs = tom_proc(images=im, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = tom_mod(**inputs).logits\n",
    "        pred = int(logits.argmax(-1))\n",
    "        conf = torch.softmax(logits, -1)[0, pred].item()\n",
    "    return f\"{tom_mod.config.id2label[pred]}  (confidence {conf:.2f})\"\n",
    "\n",
    "# =======================\n",
    "# Paddy (Image classifier)\n",
    "# =======================\n",
    "pad_model_dir, pad_proc_dir = _resolve_img_dirs(PAD)\n",
    "pad_proc = AutoImageProcessor.from_pretrained(pad_proc_dir)\n",
    "pad_mod  = AutoModelForImageClassification.from_pretrained(pad_model_dir).eval()\n",
    "\n",
    "def pred_paddy(img):\n",
    "    im = img.convert(\"RGB\") if isinstance(img, Image.Image) else Image.fromarray(img).convert(\"RGB\")\n",
    "    inputs = pad_proc(images=im, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = pad_mod(**inputs).logits\n",
    "        pred = int(logits.argmax(-1))\n",
    "        conf = torch.softmax(logits, -1)[0, pred].item()\n",
    "    return f\"{pad_mod.config.id2label[pred]}  (confidence {conf:.2f})\"\n",
    "\n",
    "# =======================\n",
    "# NER (Hindi)\n",
    "# =======================\n",
    "# Prefer split dirs; fall back to flat if needed\n",
    "ner_tok_dir = Path(NER, \"tokenizer\") if Path(NER, \"tokenizer\").exists() else Path(NER)\n",
    "ner_model_dir = Path(NER, \"model\") if Path(NER, \"model\").exists() else Path(NER)\n",
    "\n",
    "ner_tok = AutoTokenizer.from_pretrained(str(ner_tok_dir), use_fast=True)\n",
    "ner_mod = AutoModelForTokenClassification.from_pretrained(str(ner_model_dir)).eval()\n",
    "id2label = ner_mod.config.id2label\n",
    "\n",
    "def ner_hi(text):\n",
    "    enc = ner_tok(text, return_offsets_mapping=True, return_tensors=\"pt\", truncation=True)\n",
    "    offsets = enc.pop(\"offset_mapping\")[0].tolist()\n",
    "    with torch.no_grad():\n",
    "        pred_ids = ner_mod(**enc).logits.argmax(-1)[0].tolist()\n",
    "    labels = [id2label[i] for i in pred_ids]\n",
    "\n",
    "    # Merge to spans\n",
    "    spans, cur = [], None\n",
    "    for (s, e), lab in zip(offsets, labels):\n",
    "        if s == e:  # specials\n",
    "            continue\n",
    "        if lab == \"O\":\n",
    "            if cur: spans.append(cur); cur = None\n",
    "            continue\n",
    "        tag = lab.split(\"-\", 1)[-1]\n",
    "        if cur and cur[\"tag\"] == tag and s == cur[\"end\"]:\n",
    "            cur[\"end\"] = e\n",
    "        else:\n",
    "            if cur: spans.append(cur)\n",
    "            cur = {\"start\": s, \"end\": e, \"tag\": tag}\n",
    "    if cur: spans.append(cur)\n",
    "    ents = [(text[x[\"start\"]:x[\"end\"]], x[\"tag\"]) for x in spans]\n",
    "    return ents if ents else \"No entities\"\n",
    "\n",
    "# =======================\n",
    "# Q&A (AgroQA) \u2014 prefer FLAN if available, else mT5\n",
    "# =======================\n",
    "qa_dir = QA_FLAN if Path(QA_FLAN, \"model\").exists() else QA_MT5\n",
    "qa_tok_dir = Path(qa_dir, \"tokenizer\") if Path(qa_dir, \"tokenizer\").exists() else Path(qa_dir)\n",
    "qa_model_dir = Path(qa_dir, \"model\") if Path(qa_dir, \"model\").exists() else Path(qa_dir)\n",
    "\n",
    "qa_tok = AutoTokenizer.from_pretrained(str(qa_tok_dir))\n",
    "qa_mod = AutoModelForSeq2SeqLM.from_pretrained(str(qa_model_dir)).eval()\n",
    "\n",
    "# mT5 needs safer generation settings\n",
    "qa_mod.config.pad_token_id = qa_tok.pad_token_id\n",
    "qa_mod.config.eos_token_id = qa_tok.eos_token_id\n",
    "bad = None\n",
    "if \"mt5\" in qa_tok.name_or_path.lower() or \"agroqa_mt5\" in str(qa_dir).lower():\n",
    "    qa_mod.config.decoder_start_token_id = qa_tok.pad_token_id\n",
    "    bad = [[qa_tok.convert_tokens_to_ids(f\"<extra_id_{i}>\")] for i in range(100)]\n",
    "\n",
    "def ask(q):\n",
    "    q = \"question: \" + q\n",
    "    x = qa_tok(q, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "    with torch.no_grad():\n",
    "        y = qa_mod.generate(\n",
    "            **x,\n",
    "            max_new_tokens=80,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "            bad_words_ids=bad,\n",
    "            eos_token_id=qa_tok.eos_token_id,\n",
    "            pad_token_id=qa_tok.pad_token_id\n",
    "        )\n",
    "    return qa_tok.decode(y[0], skip_special_tokens=True)\n",
    "\n",
    "# =======================\n",
    "# Gradio UI\n",
    "# =======================\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# AgriVerse (AVA) \u2014 Demo\")\n",
    "    with gr.Tab(\"Tomato disease\"):\n",
    "        img_t = gr.Image(label=\"Upload tomato leaf\")\n",
    "        out_t = gr.Textbox(label=\"Prediction\")\n",
    "        img_t.change(pred_tomato, img_t, out_t)\n",
    "        gr.Button(\"Predict\").click(pred_tomato, img_t, out_t)\n",
    "\n",
    "    with gr.Tab(\"Paddy disease\"):\n",
    "        img_p = gr.Image(label=\"Upload paddy leaf\")\n",
    "        out_p = gr.Textbox(label=\"Prediction\")\n",
    "        img_p.change(pred_paddy, img_p, out_p)\n",
    "        gr.Button(\"Predict\").click(pred_paddy, img_p, out_p)\n",
    "\n",
    "    with gr.Tab(\"NER (Hindi)\"):\n",
    "        txt_n = gr.Textbox(label=\"Type Hindi sentence\")\n",
    "        out_n = gr.HighlightedText(label=\"Entities\")\n",
    "        gr.Button(\"Extract\").click(ner_hi, txt_n, out_n)\n",
    "\n",
    "    with gr.Tab(\"Agri Q&A\"):\n",
    "        txt_q = gr.Textbox(label=\"Ask a question\")\n",
    "        out_q = gr.Textbox(label=\"Answer\")\n",
    "        gr.Button(\"Answer\").click(ask, txt_q, out_q)\n",
    "\n",
    "demo.launch(share=True)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "glGD6JJ_5-qJ",
    "outputId": "778e083c-d95c-4ca8-bde7-5f84685663cf"
   },
   "id": "glGD6JJ_5-qJ",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://0e963f7adc708db56f.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"https://0e963f7adc708db56f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "execution_count": 19
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!mkdir -p /content/ava_models\n",
    "for p in [\"/content/tomato_vit\",\"/content/paddy_vit\",\"/content/naamapadam_hi_ner\",\"/content/agroqa_mt5\",\"/content/agroqa_flan\"]:\n",
    "    import os, shutil\n",
    "    if os.path.exists(p): shutil.copytree(p, f\"/content/ava_models/{p.split('/')[-1]}\", dirs_exist_ok=True)\n",
    "!zip -qr /content/ava_models.zip /content/ava_models\n",
    "from google.colab import files; files.download(\"/content/ava_models.zip\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "QgGAs9lz6u5o",
    "outputId": "a60db453-4d6c-480e-b041-e35195dd2a49"
   },
   "id": "QgGAs9lz6u5o",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ],
      "application/javascript": [
       "download(\"download_44b263b6-a475-44ca-afad-d33d5369743a\", \"ava_models.zip\", 188)"
      ]
     },
     "metadata": {}
    }
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "accelerator": "GPU",
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}